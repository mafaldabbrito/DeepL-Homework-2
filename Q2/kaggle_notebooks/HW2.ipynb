{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14381887,"sourceType":"datasetVersion","datasetId":9184749}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## Config\nfrom dataclasses import dataclass\n\n@dataclass\nclass RNAConfig:\n    \"\"\"Global configuration for the RNAcompete Data Pipeline.\"\"\"\n    \n    # Data Path\n    DATA_PATH: str = \"norm_data.txt\" #NOTE: Only change this if you want to use a different path\n    \n    # Metadata is an Excel file\n    METADATA_PATH: str = \"metadata.xlsx\" #NOTE: Only change this if you want to use a different path\n    METADATA_SHEET: str = \"Master List--Plasmid Info\"\n    \n    # Save Path\n    SAVE_DIR: str = \"data\" #NOTE: Only change this if you want to use a different path\n    \n    # Sequence Parameters\n    SEQ_MAX_LEN: int = 41\n    ALPHABET: str = \"ACGUN\"\n    \n    # Preprocessing\n    CLIP_PERCENTILE: float = 99.95\n    EPSILON: float = 1e-6  # For numerical stability\n    \n    # Split Identifiers\n    TRAIN_SPLIT_ID: str = \"SetA\"\n    TEST_SPLIT_ID: str = \"SetB\"\n    \n    VAL_SPLIT_PCT: float = 0.2\n    SEED: int = 42 # NOTE: Change this only if you want to test reproducibility","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T21:20:08.102002Z","iopub.execute_input":"2026-01-08T21:20:08.102300Z","iopub.status.idle":"2026-01-08T21:20:08.108473Z","shell.execute_reply.started":"2026-01-08T21:20:08.102280Z","shell.execute_reply":"2026-01-08T21:20:08.107860Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"## Utils\nimport os\nimport random\nimport sys\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom typing import List, Tuple\n\n\ndef configure_seed(seed):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n\nclass RNACompeteLoader:\n    def __init__(self, config: RNAConfig):\n        \"\"\"\n        Initializes the loader.\n        \"\"\"\n        self.cfg = config\n        self.meta_df = None\n        self.data_df = None\n        self.protein_to_id = None\n\n        # Setup Encoding\n        self.char_map = {\n            'A': np.array([1, 0, 0, 0], dtype=np.float32),\n            'C': np.array([0, 1, 0, 0], dtype=np.float32),\n            'G': np.array([0, 0, 1, 0], dtype=np.float32),\n            'U': np.array([0, 0, 0, 1], dtype=np.float32),\n            'N': np.array([0.25, 0.25, 0.25, 0.25], dtype=np.float32)\n        }\n        self.padding_vec = np.zeros(4, dtype=np.float32)\n\n    def _ensure_data_loaded(self):\n        \"\"\"Helper to load the heavy files only when necessary.\"\"\"\n        if self.data_df is not None:\n            return\n        \n        # Load Metadata\n        print(f\"Loading Metadata from {self.cfg.METADATA_PATH}...\")\n        start_time = time.time()\n        try:\n            if self.cfg.METADATA_PATH.endswith('.xlsx'):\n                # Requires 'openpyxl' installed!\n                self.meta_df = pd.read_excel(\n                    self.cfg.METADATA_PATH, \n                    sheet_name=self.cfg.METADATA_SHEET\n                )\n            else:\n                self.meta_df = pd.read_csv(self.cfg.METADATA_PATH)\n        except Exception as e:\n            print(f\"Error loading metadata: {e}\")\n            raise e\n        print(f\"  > Metadata loaded in {time.time() - start_time:.2f} seconds.\")\n\n        # Clean column names (strip whitespace)\n        self.meta_df.columns = [c.strip() for c in self.meta_df.columns]\n        \n        # Create Protein Name -> RNCMPT ID mapping\n        self.protein_to_id = pd.Series(\n            self.meta_df['Motif_ID'].values, \n            index=self.meta_df['Protein_name']\n        ).to_dict()\n        \n        # Load Data \n        print(f\"Loading Data from {self.cfg.DATA_PATH}...\")\n        start_time = time.time()\n\n        # standard RNAcompete is tab-separated\n        self.data_df = pd.read_csv(self.cfg.DATA_PATH, sep='\\t', low_memory=False)    \n        print(f\"  > Data Matrix loaded in {time.time() - start_time:.2f} seconds.\")\n\n        # Clean data columns\n        self.data_df.columns = [c.strip() for c in self.data_df.columns]\n\n    def list_proteins(self) -> List[str]:\n        \"\"\"Returns a sorted list of available protein names.\"\"\"\n        self._ensure_data_loaded()\n        valid_proteins = []\n        matrix_cols = set(self.data_df.columns)\n        \n        for name, pid in self.protein_to_id.items():\n            if pid in matrix_cols:\n                valid_proteins.append(name)\n        \n        return sorted(valid_proteins)\n\n    def _encode_sequence(self, seq: str) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"One-hot encodes a single RNA sequence.\"\"\"\n        # Handle NaN or non-string sequence entries gracefully\n        if not isinstance(seq, str):\n            seq = \"N\" * self.cfg.SEQ_MAX_LEN\n        \n        seq = seq.upper()[:self.cfg.SEQ_MAX_LEN]\n        \n        encoded = np.zeros((self.cfg.SEQ_MAX_LEN, len(self.padding_vec)), dtype=np.float32)\n        mask = np.zeros(self.cfg.SEQ_MAX_LEN, dtype=np.float32)\n        \n        for i, base in enumerate(seq):\n            encoded[i] = self.char_map.get(base, self.char_map['N'])\n            mask[i] = 1.0\n        \n        return encoded, mask\n    \n    def _preprocess_intensities(self, intensities: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Applies: Mask NaNs -> Clip -> Log -> Z-score.\"\"\"\n        mask = (~np.isnan(intensities)).astype(np.float32)\n        clean_vals = np.nan_to_num(intensities, nan=0.0)\n        \n        # Clip\n        if np.sum(mask) > 0:\n            valid_data = intensities[mask == 1]\n            clip_val = np.percentile(valid_data, self.cfg.CLIP_PERCENTILE)\n            clean_vals = np.clip(clean_vals, None, clip_val)\n\n        # Log Transform (Shift to positive)\n        min_val = np.min(clean_vals)\n        shift = 0\n        if min_val <= 0:\n            shift = abs(min_val) + 1.0\n        clean_vals = np.log(clean_vals + shift + self.cfg.EPSILON)\n        \n        # Z-Score\n        masked_vals = clean_vals[mask == 1]\n        if len(masked_vals) > 0:\n            mean = np.mean(masked_vals)\n            std = np.std(masked_vals) + self.cfg.EPSILON\n            clean_vals = (clean_vals - mean) / std\n        \n        clean_vals = clean_vals * mask\n        return clean_vals, mask\n    \n    def get_data(self, protein_name: str, split: str = 'train') -> TensorDataset:\n        \"\"\"\n        Main method to get PyTorch Dataset for a specific protein.\n        \"\"\"\n        # Fast path: use preprocessed tensors if they are already available (e.g., Kaggle input dir)\n        kaggle_path = f\"/kaggle/input/rnacompete/{protein_name}_{split}_data.pt\"\n        if os.path.exists(kaggle_path):\n            print(f\"Found preprocessed data at {kaggle_path}. Loading...\")\n            tensors = torch.load(kaggle_path, weights_only=True)\n            return TensorDataset(*tensors)\n\n        # Check cache first\n        os.makedirs(self.cfg.SAVE_DIR, exist_ok=True)\n        data_path = os.path.join(self.cfg.SAVE_DIR, f\"{protein_name}_{split}_data.pt\")\n\n        if os.path.exists(data_path):\n            print(f\"Found cached data for {protein_name} ({split}). Loading from {data_path}...\")\n            try:\n                tensors = torch.load(data_path, weights_only=True)\n                return TensorDataset(*tensors)\n            except Exception as e:\n                print(f\"Cache seems corrupted: {e}. Will reload from scratch.\")\n\n        self._ensure_data_loaded()\n\n        if protein_name not in self.protein_to_id:\n            raise ValueError(f\"Protein '{protein_name}' not found in metadata.\")\n        \n        rncmpt_id = self.protein_to_id[protein_name]\n        \n        if rncmpt_id not in self.data_df.columns:\n            raise ValueError(f\"ID {rncmpt_id} for {protein_name} missing from data matrix.\")\n\n        s_lower = split.lower()\n\n        if s_lower == 'test':\n            # Test set is just SetB, nice and simple\n            subset = self.data_df[self.data_df['Probe_Set'] == self.cfg.TEST_SPLIT_ID].copy()\n\n        elif s_lower in ['train', 'val']:\n            # For train/val, we need to split SetA. \n            # We use a fixed seed to ensure grading consistency (everyone gets the same split).\n            full_set = self.data_df[self.data_df['Probe_Set'] == self.cfg.TRAIN_SPLIT_ID]\n            \n            # Explicitly sort by index to ensure deterministic order before shuffling\n            full_set = full_set.sort_index()\n            \n            n_samples = len(full_set)\n            indices = np.arange(n_samples)\n            \n            # Local RandomState prevents messing with global seeds\n            rng = np.random.RandomState(self.cfg.SEED)\n            rng.shuffle(indices)\n            \n            val_size = int(n_samples * self.cfg.VAL_SPLIT_PCT)\n            \n            if s_lower == 'val':\n                # Validation gets the first chunk\n                subset_indices = indices[:val_size]\n            else:\n                # Train gets the leftovers\n                subset_indices = indices[val_size:]\n                \n            subset = full_set.iloc[subset_indices].copy()\n        else:\n            raise ValueError(f\"Unknown split '{split}'. Please use 'train', 'val', or 'test'.\")\n        \n        # Extract Sequences\n        raw_seqs = subset['RNA_Seq'].values\n        encoded_list = []\n        mask_list = []\n\n        for s in raw_seqs:\n            encoded, seq_mask = self._encode_sequence(s) \n            encoded_list.append(encoded)\n            mask_list.append(seq_mask)\n\n        X = np.stack(encoded_list)          # shape: (B, SEQ_MAX_LEN, 4)\n        sequence_masks = np.stack(mask_list)  # shape: (B, SEQ_MAX_LEN)\n\n        # Process Intensities\n        # Force conversion to numeric (floats), turning any strings/errors into NaN\n        raw_intensities = pd.to_numeric(subset[rncmpt_id], errors='coerce').values\n        Y, mask = self._preprocess_intensities(raw_intensities)\n        \n        # Convert to Tensor\n        dataset = TensorDataset(\n            torch.FloatTensor(X),                     # (B, 41, 4)\n            torch.FloatTensor(sequence_masks),        # (B, 41)\n            torch.FloatTensor(Y).unsqueeze(1),        # (B, 1)\n            torch.FloatTensor(mask).unsqueeze(1)      # (B, 1)\n        )\n        \n        # Save for next time\n        print(f\"Saving processed data to {data_path}...\")\n        torch.save(dataset.tensors, data_path)\n        \n        return dataset\n    \n\ndef load_rnacompete_data(protein_name: str, split: str = 'train', config: RNAConfig = None):\n    \"\"\"\n    Convenience function to load data for a single protein without manually managing the loader class.\n    Note: Instantiates the loader from scratch (loads files). \n    For bulk processing, use RNACompeteLoader class directly.\n    \"\"\"\n    if config is None:\n        config = RNAConfig()\n\n    loader = RNACompeteLoader(config)\n    return loader.get_data(protein_name, split)\n\n\ndef masked_spearman_correlation(preds, targets, masks):\n    \"\"\"\n    Calculates Spearman Rank Correlation on masked data.\n    Expects:\n        preds: (B, 1)\n        targets: (B, 1)\n        masks: (B, 1)\n    Outputs:\n        correlation: scalar\n    \"\"\"\n    # Flatten and detach (metrics don't need gradients)\n    preds = preds.squeeze().detach()\n    targets = targets.squeeze().detach()\n    masks = masks.squeeze().bool()\n    \n    valid_preds = preds[masks]\n    valid_targets = targets[masks]\n    \n    if valid_preds.numel() < 2:\n        return torch.tensor(0.0)\n\n    # argsort twice gets us the ranks\n    pred_ranks = valid_preds.argsort().argsort().float()\n    target_ranks = valid_targets.argsort().argsort().float()\n\n    # Pearson on ranks == Spearman\n    pred_mean = pred_ranks.mean()\n    target_mean = target_ranks.mean()\n\n    pred_var = pred_ranks - pred_mean\n    target_var = target_ranks - target_mean\n\n    correlation = (pred_var * target_var).sum() / torch.sqrt((pred_var ** 2).sum() * (target_var ** 2).sum())\n\n    return correlation\n\n\ndef masked_mse_loss(preds, targets, masks):\n    \"\"\"\n    Calculates Mean Squared Error, ignoring padded elements.\n    Expects:\n        preds: (B, 1)\n        targets: (B, 1)\n        masks: (B, 1)\n    Outputs:\n        loss: scalar\n    \"\"\"\n    # Flatten to 1D\n    preds = preds.squeeze()\n    targets = targets.squeeze()\n    masks = masks.squeeze().bool()\n\n    # Filter out padded values\n    masked_preds = preds[masks]\n    masked_targets = targets[masks]\n    \n    # Handle empty batch case\n    if masked_preds.numel() == 0:\n        return torch.tensor(0.0, device=preds.device, requires_grad=True)\n\n    # MSE on valid data\n    squared_error = (masked_preds - masked_targets) ** 2\n    loss = torch.mean(squared_error)\n    \n    return loss\n\n\ndef plot(epochs, plottables, filename=None, ylim=None):\n    \"\"\"Plot the plottables over the epochs.\n    \n    Plottables is a dictionary mapping labels to lists of values.\n    \"\"\"\n    plt.clf()\n    plt.xlabel('Epoch')\n    for label, plottable in plottables.items():\n        plt.plot(epochs, plottable, label=label)\n    plt.legend()\n    if ylim:\n        plt.ylim(ylim)\n    if filename:\n        plt.savefig(filename, bbox_inches='tight')\n\n\n# ---------------------------------------------------------------------------\n# Data exploration helpers\n# ---------------------------------------------------------------------------\n\ndef dataset_summary(dataset: TensorDataset) -> dict:\n    \"\"\"Return simple stats about a TensorDataset (X, y, mask).\"\"\"\n    X, y, mask = dataset.tensors\n    mask_bool = mask.squeeze().bool()\n\n    # Masked targets only\n    valid_targets = y.squeeze()[mask_bool]\n    valid_np = valid_targets.detach().cpu().numpy() if valid_targets.numel() else np.array([])\n\n    # Sequence lengths (after padding removal)\n    seq_lengths = (X.sum(dim=2) > 0).sum(dim=1).detach().cpu().numpy()\n\n    return {\n        \"num_samples\": int(X.shape[0]),\n        \"seq_min_len\": int(seq_lengths.min()) if len(seq_lengths) else 0,\n        \"seq_max_len\": int(seq_lengths.max()) if len(seq_lengths) else 0,\n        \"seq_mean_len\": float(seq_lengths.mean()) if len(seq_lengths) else 0.0,\n        \"mask_fraction\": float(mask_bool.float().mean().item()),\n        \"target_min\": float(valid_np.min()) if valid_np.size else 0.0,\n        \"target_max\": float(valid_np.max()) if valid_np.size else 0.0,\n        \"target_mean\": float(valid_np.mean()) if valid_np.size else 0.0,\n        \"target_std\": float(valid_np.std()) if valid_np.size else 0.0,\n    }\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-08T21:02:22.169957Z","iopub.execute_input":"2026-01-08T21:02:22.170231Z","iopub.status.idle":"2026-01-08T21:02:22.325128Z","shell.execute_reply.started":"2026-01-08T21:02:22.170210Z","shell.execute_reply":"2026-01-08T21:02:22.324327Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"## RNN\n\"\"\"\nCNN + BiLSTM model for RNAcompete binding prediction.\n\nMotifs are detected by a convolutional layer, and a bidirectional LSTM\nmodels upstream/downstream context before fully connected layers map\nfeatures to binding affinity.\n\"\"\"\n\nimport argparse\nimport json\nimport os\nimport time\nfrom typing import Dict, List, Tuple, Optional\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom argparse import Namespace\n\n# from utils import (\n# \tRNACompeteLoader,\n# \tconfigure_seed,\n# \tmasked_mse_loss,\n# \tmasked_spearman_correlation,\n# \tplot,\n# \tdataset_summary,\n# )\n# from config import RNAConfig\n\n\n# ============================================================================\n# Model: CNN -> BiLSTM -> FC\n# ============================================================================\n\n\nclass RNABindingCNNBiLSTM(nn.Module):\n\t\"\"\"Detect local motifs with CNN then model context with BiLSTM.\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\tnum_filters: int = 64,\n\t\tkernel_size: int = 8,\n\t\tlstm_hidden: int = 64,\n\t\tlstm_layers: int = 1,\n\t\tdropout_rate: float = 0.3,\n\t\tinput_channels: int = 4,\n\t\tseq_length: int = 41,\n\t):\n\t\tsuper().__init__()\n\n\t\tself.seq_length = seq_length\n\t\tself.num_directions = 2  # bidirectional\n\t\tself.lstm_layers = lstm_layers\n\t\tself.lstm_hidden = lstm_hidden\n\n\t\t# Motif detector\n\t\tself.conv1 = nn.Conv1d(\n\t\t\tin_channels=input_channels,\n\t\t\tout_channels=num_filters,\n\t\t\tkernel_size=kernel_size,\n\t\t\tstride=1,\n\t\t\tpadding=kernel_size // 2,\n\t\t)\n\n\t\t# Batch Normalization after convolution for better regularization\n\t\tself.bn1 = nn.BatchNorm1d(num_filters)\n\n\t\tself.dropout_conv = nn.Dropout(dropout_rate * 0.5)\n\n\t\t# Context encoder\n\t\tself.lstm = nn.LSTM(\n\t\t\tinput_size=num_filters,\n\t\t\thidden_size=lstm_hidden,\n\t\t\tnum_layers=lstm_layers,\n\t\t\tbatch_first=True,\n\t\t\tbidirectional=True,\n\t\t)\n\n\t\t# FC head\n\t\tfc_in = lstm_hidden * self.num_directions\n\t\tself.fc1 = nn.Linear(fc_in, 128)\n\t\tself.bn_fc1 = nn.BatchNorm1d(128)\n\t\tself.fc2 = nn.Linear(128, 64)\n\t\tself.bn_fc2 = nn.BatchNorm1d(64)\n\t\tself.fc3 = nn.Linear(64, 1)\n\n\t\tself.dropout = nn.Dropout(dropout_rate)\n\n\tdef forward(self, x: torch.Tensor) -> torch.Tensor:\n\t\t# x: (batch, seq_len, 4)\n\t\tx = x.transpose(1, 2)  # -> (batch, 4, seq_len)\n\t\tx = self.conv1(x)\n\t\tx = self.bn1(x)\n\t\tx = F.relu(x)  # -> (batch, filters, seq_len)\n\t\tx = self.dropout_conv(x)\n\n\t\t# Prepare for LSTM\n\t\tx = x.transpose(1, 2)  # -> (batch, seq_len, filters)\n\t\tlstm_out, (h_n, _) = self.lstm(x)\n\n\t\t# Take last-layer forward/backward states and concatenate\n\t\th_n = h_n.view(self.lstm_layers, self.num_directions, x.size(0), self.lstm_hidden)\n\t\tforward_last = h_n[-1, 0]\n\t\tbackward_last = h_n[-1, 1]\n\t\tcontextual = torch.cat([forward_last, backward_last], dim=1)\n\n\t\tx = self.fc1(contextual)\n\t\tx = self.bn_fc1(x)\n\t\tx = F.relu(x)\n\t\tx = self.dropout(x)\n\n\t\tx = self.fc2(x)\n\t\tx = self.bn_fc2(x)\n\t\tx = F.relu(x)\n\t\tx = self.dropout(x)\n\n\t\tx = self.fc3(x)\n\t\treturn x\n\n\n# ============================================================================\n# Training / evaluation helpers\n# ============================================================================\n\n\ndef train_epoch(\n\tmodel: nn.Module,\n\ttrain_loader: DataLoader,\n\toptimizer: optim.Optimizer,\n\tdevice: torch.device,\n) -> Tuple[float, float]:\n\tmodel.train()\n\ttotal_loss = 0.0\n\ttotal_corr = 0.0\n\tnum_batches = 0\n\n\tfor batch_idx, (sequences, targets, masks) in enumerate(train_loader):\n\t\tsequences = sequences.to(device)\n\t\ttargets = targets.to(device)\n\t\tmasks = masks.to(device)\n\n\t\toptimizer.zero_grad()\n\n\t\tpredictions = model(sequences)\n\t\tloss = masked_mse_loss(predictions, targets, masks)\n\t\tloss.backward()\n\t\toptimizer.step()\n\n\t\twith torch.no_grad():\n\t\t\tcorr = masked_spearman_correlation(predictions, targets, masks)\n\n\t\ttotal_loss += loss.item()\n\t\ttotal_corr += corr.item()\n\t\tnum_batches += 1\n\n\t\t# if (batch_idx + 1) % 50 == 0:\n\t\t# \tprint(\n\t\t# \t\tf\"  Batch {batch_idx + 1}/{len(train_loader)}: \"\n\t\t# \t\tf\"Loss = {loss.item():.4f}, Corr = {corr.item():.4f}\"\n\t\t# \t)\n\n\tavg_loss = total_loss / num_batches\n\tavg_corr = total_corr / num_batches\n\treturn avg_loss, avg_corr\n\n\ndef evaluate(\n\tmodel: nn.Module,\n\tdata_loader: DataLoader,\n\tdevice: torch.device,\n) -> Tuple[float, float]:\n\tmodel.eval()\n\ttotal_loss = 0.0\n\ttotal_corr = 0.0\n\tnum_batches = 0\n\n\twith torch.no_grad():\n\t\tfor sequences, targets, masks in data_loader:\n\t\t\tsequences = sequences.to(device)\n\t\t\ttargets = targets.to(device)\n\t\t\tmasks = masks.to(device)\n\n\t\t\tpredictions = model(sequences)\n\t\t\tloss = masked_mse_loss(predictions, targets, masks)\n\t\t\tcorr = masked_spearman_correlation(predictions, targets, masks)\n\n\t\t\ttotal_loss += loss.item()\n\t\t\ttotal_corr += corr.item()\n\t\t\tnum_batches += 1\n\n\tavg_loss = total_loss / num_batches\n\tavg_corr = total_corr / num_batches\n\treturn avg_loss, avg_corr\n\n\n\ndef train_model(\n\tmodel: nn.Module,\n\ttrain_loader: DataLoader,\n\tval_loader: DataLoader,\n\tnum_epochs: int,\n\tlearning_rate: float,\n\tdevice: torch.device,\n\tsave_path: Optional[str] = None,\n\tweight_decay: float = 1e-3,\n\tearly_stopping_patience: int = 5\n) -> Dict[str, List[float]]:\n\tstart_time = time.time()\n\t\n\tprint(f\"\\n{'='*70}\")\n\tprint(f\"Starting Training on {device}\")\n\tprint(f\"{'='*70}\")\n\tprint(f\"Regularization: Weight Decay = {weight_decay}, Early Stopping Patience = {early_stopping_patience}\")\n\n\toptimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n\n\thistory: Dict[str, List[float]] = {\n\t\t\"train_loss\": [],\n\t\t\"train_corr\": [],\n\t\t\"val_loss\": [],\n\t\t\"val_corr\": [],\n\t}\n\n\tbest_val_corr = -1.0\n\tpatience_counter = 0\n\n\tfor epoch in range(num_epochs):\n\t\tprint(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n\t\tprint(f\"{'-'*70}\")\n\n\t\ttrain_loss, train_corr = train_epoch(model, train_loader, optimizer, device)\n\t\tval_loss, val_corr = evaluate(model, val_loader, device)\n\n\t\thistory[\"train_loss\"].append(train_loss)\n\t\thistory[\"train_corr\"].append(train_corr)\n\t\thistory[\"val_loss\"].append(val_loss)\n\t\thistory[\"val_corr\"].append(val_corr)\n\n\t\tprint(\n\t\t\tf\"\\nEpoch {epoch + 1} Summary:\\n\"\n\t\t\tf\"  Train Loss: {train_loss:.4f} | Train Corr: {train_corr:.4f}\\n\"\n\t\t\tf\"  Val Loss:   {val_loss:.4f} | Val Corr:   {val_corr:.4f}\"\n\t\t)\n\n\t\tif val_corr > best_val_corr:\n\t\t\tbest_val_corr = val_corr\n\t\t\tpatience_counter = 0\n\t\t\tif save_path:\n\t\t\t\ttorch.save(\n\t\t\t\t\t{\n\t\t\t\t\t\t\"epoch\": epoch,\n\t\t\t\t\t\t\"model_state_dict\": model.state_dict(),\n\t\t\t\t\t\t\"optimizer_state_dict\": optimizer.state_dict(),\n\t\t\t\t\t\t\"val_corr\": val_corr,\n\t\t\t\t\t},\n\t\t\t\t\tsave_path,\n\t\t\t\t)\n\t\t\t\tprint(f\"  >>> New best model saved! (Val Corr: {val_corr:.4f})\")\n\t\telse:\n\t\t\tpatience_counter += 1\n\t\t\tprint(f\"  No improvement. Patience: {patience_counter}/{early_stopping_patience}\")\n\t\t\tif patience_counter >= early_stopping_patience:\n\t\t\t\tprint(f\"\\n{'='*70}\")\n\t\t\t\tprint(f\"Early stopping triggered after {epoch + 1} epochs\")\n\t\t\t\tprint(f\"{'='*70}\\n\")\n\t\t\t\tbreak\n\n\telapsed_time = time.time() - start_time\n\t\n\tprint(f\"\\n{'='*70}\")\n\tprint(f\"Training Complete! Best Val Correlation: {best_val_corr:.4f}\")\n\tprint(f\"Total Training Time: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n\tprint(f\"{'='*70}\\n\")\n\n\thistory['elapsed_time'] = elapsed_time\n\treturn history\n\n\n# ============================================================================\n# Hyperparameter search (small, curated)\n# ============================================================================\n\n\ndef hyperparameter_search(\n\tprotein_name: str,\n\tconfig: RNAConfig,\n\tdevice: torch.device,\n\tbatch_size: int = 64,\n\tnum_epochs: int = 15,\n) -> Tuple[Dict, List[Dict]]:\n\tprint(f\"\\n{'='*70}\")\n\tprint(f\"HYPERPARAMETER SEARCH FOR {protein_name}\")\n\tprint(f\"{'='*70}\\n\")\n\n\tprint(\"Loading data (this will take a minute or two)...\")\n\tloader = RNACompeteLoader(config)\n\ttrain_dataset = loader.get_data(protein_name, split=\"train\")\n\tval_dataset = loader.get_data(protein_name, split=\"val\")\n\n\ttrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\tval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n\ttest_configs = [       \n\t\t{\"num_filters\": 96, \"kernel_size\": 8, \"lstm_hidden\": 96,  \"lstm_layers\": 1, \"dropout_rate\": 0.5, \"learning_rate\": 5e-4},\n\t\t# Slightly higher capacity\n\t\t{\"num_filters\": 96, \"kernel_size\": 8, \"lstm_hidden\": 128, \"lstm_layers\": 1, \"dropout_rate\": 0.5, \"learning_rate\": 5e-4},\n\t\t{\"num_filters\": 96, \"kernel_size\": 8, \"lstm_hidden\": 64, \"lstm_layers\": 2, \"dropout_rate\": 0.5, \"learning_rate\": 5e-4},\n    \t{\"num_filters\": 96, \"kernel_size\": 8, \"lstm_hidden\": 96, \"lstm_layers\": 2, \"dropout_rate\": 0.5, \"learning_rate\": 5e-4},\n    ]\n\n\tbest_config: Optional[Dict] = None\n\tbest_val_corr = -1.0\n\tall_results: List[Dict] = []\n\n\tfor idx, params in enumerate(test_configs):\n\t\tprint(f\"\\n{'='*70}\")\n\t\tprint(f\"Configuration {idx + 1}/{len(test_configs)}\")\n\t\tprint(f\"{'='*70}\")\n\t\tprint(f\"Parameters: {params}\")\n\n\t\tmodel = RNABindingCNNBiLSTM(\n\t\t\tnum_filters=params[\"num_filters\"],\n\t\t\tkernel_size=params[\"kernel_size\"],\n\t\t\tlstm_hidden=params[\"lstm_hidden\"],\n\t\t\tlstm_layers=params[\"lstm_layers\"],\n\t\t\tdropout_rate=params[\"dropout_rate\"],\n\t\t).to(device)\n\n\t\thistory = train_model(\n\t\t\tmodel=model,\n\t\t\ttrain_loader=train_loader,\n\t\t\tval_loader=val_loader,\n\t\t\tnum_epochs=num_epochs,\n\t\t\tlearning_rate=params[\"learning_rate\"],\n\t\t\tdevice=device,\n\t\t\tsave_path=None,\n\t\t)\n\n\t\tfinal_val_corr = history[\"val_corr\"][-1]\n\t\tall_results.append({\"params\": params, \"final_val_corr\": final_val_corr})\n\n\t\tif final_val_corr > best_val_corr:\n\t\t\tbest_val_corr = final_val_corr\n\t\t\tbest_config = params.copy()\n\t\t\tprint(f\"\\n>>> NEW BEST CONFIGURATION! Val Corr: {final_val_corr:.4f}\")\n\n\tprint(f\"\\n{'='*70}\")\n\tprint(\"HYPERPARAMETER SEARCH COMPLETE\")\n\tprint(f\"{'='*70}\\n\")\n\tprint(\"Best Configuration:\")\n\tfor k, v in best_config.items():\n\t\tprint(f\"  {k}: {v}\")\n\tprint(f\"\\nBest Validation Correlation: {best_val_corr:.4f}\")\n\n\treturn best_config, all_results\n\n\n# ============================================================================\n# Data exploration helper\n# ============================================================================\n\n\ndef run_data_exploration(\n\tprotein_name: str,\n\tconfig: RNAConfig,\n\toutput_dir: str = \"results\",\n\tsplits: Tuple[str, ...] = (\"train\", \"val\", \"test\"),\n) -> Dict[str, Dict]:\n\tos.makedirs(output_dir, exist_ok=True)\n\tloader = RNACompeteLoader(config)\n\n\tsummaries: Dict[str, Dict] = {}\n\tfor split in splits:\n\t\tprint(f\"Loading {protein_name} {split} split for exploration...\")\n\t\tdataset = loader.get_data(protein_name, split=split)\n\t\tsummary = dataset_summary(dataset)\n\t\tsummaries[split] = summary\n\n\t\tsummary_path = os.path.join(output_dir, f\"{protein_name}_{split}_summary.json\")\n\t\twith open(summary_path, \"w\") as f:\n\t\t\tjson.dump(summary, f, indent=2)\n\t\tprint(f\"  Saved summary to {summary_path}\")\n\n\treturn summaries\n\n\n# ============================================================================\n# Main\n# ============================================================================\n\n\ndef main():\n    # parser = argparse.ArgumentParser(description=\"RNAcompete CNN+BiLSTM trainer and data explorer\")\n    # parser.add_argument(\"--protein\", default=\"RBFOX1\", help=\"Protein name to process\")\n    # parser.add_argument(\"--batch-size\", type=int, default=64, help=\"Batch size for training\")\n    # parser.add_argument(\"--epochs\", type=int, default=30, help=\"Number of training epochs\")\n    # parser.add_argument(\"--hyperparam-search\", action=\"store_true\", help=\"Run hyperparameter search instead of default config\")\n    # parser.add_argument(\"--explore-only\", action=\"store_true\", help=\"Only run data summary/plots and skip training\")\n    # args = parser.parse_args()\n    args = Namespace(\n        protein=\"RBFOX1\",\n        batch_size=64,\n        epochs=50,\n        hyperparam_search=False,\n        explore_only=False,\n    )\n    \n    configure_seed(42)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    config = RNAConfig()\n    os.makedirs(\"results\", exist_ok=True)\n    os.makedirs(\"models\", exist_ok=True)\n\n    if args.explore_only:\n        print(f\"\\n{'='*70}\")\n        print(f\"DATA EXPLORATION FOR {args.protein}\")\n        print(f\"{'='*70}\\n\")\n        run_data_exploration(protein_name=args.protein, config=config, output_dir=\"results\")\n        print(\"Data exploration finished. Skipping training as requested.\")\n        return\n\n    if args.hyperparam_search:\n        best_config, all_results = hyperparameter_search(\n            protein_name=args.protein,\n            config=config,\n            device=device,\n            batch_size=args.batch_size,\n            num_epochs=15,\n        )\n\n        results_file = f\"results/{args.protein}_hyperparameter_search_bilstm.json\"\n        with open(results_file, \"w\") as f:\n            serializable_results = [\n                {\"params\": res[\"params\"], \"final_val_corr\": res[\"final_val_corr\"]}\n                for res in all_results\n            ]\n            json.dump({\"best_config\": best_config, \"all_results\": serializable_results}, f, indent=2)\n        print(f\"Hyperparameter search results saved to {results_file}\")\n\n    else:\n        best_config = {\n            \"num_filters\": 96,\n            \"kernel_size\": 8,\n            \"lstm_hidden\": 128,\n            \"lstm_layers\": 1,\n            \"dropout_rate\": 0.5,\n            \"learning_rate\": 5e-4,\n        }\n        print(\"\\nUsing default configuration:\")\n        for key, value in best_config.items():\n            print(f\"  {key}: {value}\")\n\n    print(f\"\\n{'='*70}\")\n    print(f\"TRAINING FINAL MODEL FOR {args.protein}\")\n    print(f\"{'='*70}\\n\")\n\n    print(\"Loading data (this will take a minute or two)...\")\n    loader = RNACompeteLoader(config)\n    train_dataset = loader.get_data(args.protein, split=\"train\")\n    val_dataset = loader.get_data(args.protein, split=\"val\")\n    test_dataset = loader.get_data(args.protein, split=\"test\")\n\n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n\n    print(f\"Train set: {len(train_dataset)} samples\")\n    print(f\"Val set:   {len(val_dataset)} samples\")\n    print(f\"Test set:  {len(test_dataset)} samples\")\n\n    model = RNABindingCNNBiLSTM(\n        num_filters=best_config[\"num_filters\"],\n        kernel_size=best_config[\"kernel_size\"],\n        lstm_hidden=best_config[\"lstm_hidden\"],\n        lstm_layers=best_config[\"lstm_layers\"],\n        dropout_rate=best_config[\"dropout_rate\"],\n    ).to(device)\n\n    print(\"\\nModel Architecture:\")\n    print(model)\n    print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n    model_save_path = f\"models/{args.protein}_best_model_bilstm.pt\"\n    history = train_model(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        num_epochs=args.epochs,\n        learning_rate=best_config[\"learning_rate\"],\n        device=device,\n        save_path=model_save_path,\n    )\n\n    print(f\"\\n{'='*70}\")\n    print(\"FINAL EVALUATION ON TEST SET\")\n    print(f\"{'='*70}\\n\")\n\n    checkpoint = torch.load(model_save_path, weights_only=True)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n\n    test_loss, test_corr = evaluate(model, test_loader, device)\n\n    print(f\"Test Loss:        {test_loss:.4f}\")\n    print(f\"Test Correlation: {test_corr:.4f}\")\n\n    final_results = {\n        \"protein\": args.protein,\n        \"config\": best_config,\n        \"test_loss\": test_loss,\n        \"test_correlation\": test_corr,\n        \"training_time_seconds\": history['elapsed_time'],\n\t\t\"training_time_minutes\": history['elapsed_time'] / 60,\n\t\t'history': {k: ([float(v) for v in vals] if isinstance(vals, list) else float(vals)) for k, vals in history.items()}\n\t}\n\n    results_file = f\"results/{args.protein}_final_results_bilstm.json\"\n    with open(results_file, \"w\") as f:\n        json.dump(final_results, f, indent=2)\n    print(f\"\\nResults saved to {results_file}\")\n\n    actual_epochs = len(history['train_loss'])\n    epochs = list(range(1, actual_epochs + 1))\n\n    plot(\n        epochs=epochs,\n        plottables={\"Train Loss\": history[\"train_loss\"], \"Val Loss\": history[\"val_loss\"]},\n        filename=f\"results/{args.protein}_loss_curve_bilstm.png\",\n    )\n    print(f\"Loss curve saved to results/{args.protein}_loss_curve_bilstm.png\")\n\n    plot(\n        epochs=epochs,\n        plottables={\"Train Correlation\": history[\"train_corr\"], \"Val Correlation\": history[\"val_corr\"]},\n        filename=f\"results/{args.protein}_correlation_curve_bilstm.png\",\n        ylim=[0, 1],\n    )\n    print(f\"Correlation curve saved to results/{args.protein}_correlation_curve_bilstm.png\")\n\n    print(f\"\\n{'='*70}\")\n    print(\"ALL DONE!\")\n    print(f\"{'='*70}\\n\")\n\n\nif __name__ == \"__main__\":\n\tmain()\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2026-01-08T21:37:34.430182Z","iopub.execute_input":"2026-01-08T21:37:34.430479Z","iopub.status.idle":"2026-01-08T21:42:35.920771Z","shell.execute_reply.started":"2026-01-08T21:37:34.430459Z","shell.execute_reply":"2026-01-08T21:42:35.920064Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Using device: cuda\n\nUsing default configuration:\n  num_filters: 96\n  kernel_size: 8\n  lstm_hidden: 128\n  lstm_layers: 1\n  dropout_rate: 0.5\n  learning_rate: 0.0005\n\n======================================================================\nTRAINING FINAL MODEL FOR RBFOX1\n======================================================================\n\nLoading data (this will take a minute or two)...\nFound preprocessed data at /kaggle/input/rnacompete/RBFOX1_train_data.pt. Loading...\nFound preprocessed data at /kaggle/input/rnacompete/RBFOX1_val_data.pt. Loading...\nFound preprocessed data at /kaggle/input/rnacompete/RBFOX1_test_data.pt. Loading...\nTrain set: 96261 samples\nVal set:   24065 samples\nTest set:  121031 samples\n\nModel Architecture:\nRNABindingCNNBiLSTM(\n  (conv1): Conv1d(4, 96, kernel_size=(8,), stride=(1,), padding=(4,))\n  (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout_conv): Dropout(p=0.25, inplace=False)\n  (lstm): LSTM(96, 128, batch_first=True, bidirectional=True)\n  (fc1): Linear(in_features=256, out_features=128, bias=True)\n  (bn_fc1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc2): Linear(in_features=128, out_features=64, bias=True)\n  (bn_fc2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc3): Linear(in_features=64, out_features=1, bias=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n)\n\nTotal parameters: 276,385\n\n======================================================================\nStarting Training on cuda\n======================================================================\nRegularization: Weight Decay = 0.001, Early Stopping Patience = 5\n\nEpoch 1/50\n----------------------------------------------------------------------\n\nEpoch 1 Summary:\n  Train Loss: 0.8206 | Train Corr: 0.3265\n  Val Loss:   0.5749 | Val Corr:   0.4928\n  >>> New best model saved! (Val Corr: 0.4928)\n\nEpoch 2/50\n----------------------------------------------------------------------\n\nEpoch 2 Summary:\n  Train Loss: 0.5771 | Train Corr: 0.4881\n  Val Loss:   0.5430 | Val Corr:   0.5411\n  >>> New best model saved! (Val Corr: 0.5411)\n\nEpoch 3/50\n----------------------------------------------------------------------\n\nEpoch 3 Summary:\n  Train Loss: 0.5264 | Train Corr: 0.5181\n  Val Loss:   0.5092 | Val Corr:   0.5619\n  >>> New best model saved! (Val Corr: 0.5619)\n\nEpoch 4/50\n----------------------------------------------------------------------\n\nEpoch 4 Summary:\n  Train Loss: 0.4955 | Train Corr: 0.5432\n  Val Loss:   0.5625 | Val Corr:   0.5520\n  No improvement. Patience: 1/5\n\nEpoch 5/50\n----------------------------------------------------------------------\n\nEpoch 5 Summary:\n  Train Loss: 0.4799 | Train Corr: 0.5556\n  Val Loss:   0.3952 | Val Corr:   0.6076\n  >>> New best model saved! (Val Corr: 0.6076)\n\nEpoch 6/50\n----------------------------------------------------------------------\n\nEpoch 6 Summary:\n  Train Loss: 0.4656 | Train Corr: 0.5611\n  Val Loss:   0.4021 | Val Corr:   0.6044\n  No improvement. Patience: 1/5\n\nEpoch 7/50\n----------------------------------------------------------------------\n\nEpoch 7 Summary:\n  Train Loss: 0.4597 | Train Corr: 0.5676\n  Val Loss:   0.3797 | Val Corr:   0.6133\n  >>> New best model saved! (Val Corr: 0.6133)\n\nEpoch 8/50\n----------------------------------------------------------------------\n\nEpoch 8 Summary:\n  Train Loss: 0.4535 | Train Corr: 0.5726\n  Val Loss:   0.4056 | Val Corr:   0.6143\n  >>> New best model saved! (Val Corr: 0.6143)\n\nEpoch 9/50\n----------------------------------------------------------------------\n\nEpoch 9 Summary:\n  Train Loss: 0.4434 | Train Corr: 0.5747\n  Val Loss:   0.3955 | Val Corr:   0.6105\n  No improvement. Patience: 1/5\n\nEpoch 10/50\n----------------------------------------------------------------------\n\nEpoch 10 Summary:\n  Train Loss: 0.4397 | Train Corr: 0.5741\n  Val Loss:   0.4755 | Val Corr:   0.6038\n  No improvement. Patience: 2/5\n\nEpoch 11/50\n----------------------------------------------------------------------\n\nEpoch 11 Summary:\n  Train Loss: 0.4310 | Train Corr: 0.5810\n  Val Loss:   0.3776 | Val Corr:   0.6175\n  >>> New best model saved! (Val Corr: 0.6175)\n\nEpoch 12/50\n----------------------------------------------------------------------\n\nEpoch 12 Summary:\n  Train Loss: 0.4282 | Train Corr: 0.5786\n  Val Loss:   0.3780 | Val Corr:   0.6186\n  >>> New best model saved! (Val Corr: 0.6186)\n\nEpoch 13/50\n----------------------------------------------------------------------\n\nEpoch 13 Summary:\n  Train Loss: 0.4303 | Train Corr: 0.5822\n  Val Loss:   0.4090 | Val Corr:   0.6143\n  No improvement. Patience: 1/5\n\nEpoch 14/50\n----------------------------------------------------------------------\n\nEpoch 14 Summary:\n  Train Loss: 0.4223 | Train Corr: 0.5830\n  Val Loss:   0.5526 | Val Corr:   0.5865\n  No improvement. Patience: 2/5\n\nEpoch 15/50\n----------------------------------------------------------------------\n\nEpoch 15 Summary:\n  Train Loss: 0.4216 | Train Corr: 0.5857\n  Val Loss:   0.3843 | Val Corr:   0.6214\n  >>> New best model saved! (Val Corr: 0.6214)\n\nEpoch 16/50\n----------------------------------------------------------------------\n\nEpoch 16 Summary:\n  Train Loss: 0.4204 | Train Corr: 0.5860\n  Val Loss:   0.3765 | Val Corr:   0.6218\n  >>> New best model saved! (Val Corr: 0.6218)\n\nEpoch 17/50\n----------------------------------------------------------------------\n\nEpoch 17 Summary:\n  Train Loss: 0.4223 | Train Corr: 0.5840\n  Val Loss:   0.3835 | Val Corr:   0.6252\n  >>> New best model saved! (Val Corr: 0.6252)\n\nEpoch 18/50\n----------------------------------------------------------------------\n\nEpoch 18 Summary:\n  Train Loss: 0.4155 | Train Corr: 0.5855\n  Val Loss:   0.4033 | Val Corr:   0.6119\n  No improvement. Patience: 1/5\n\nEpoch 19/50\n----------------------------------------------------------------------\n\nEpoch 19 Summary:\n  Train Loss: 0.4174 | Train Corr: 0.5857\n  Val Loss:   0.3641 | Val Corr:   0.6237\n  No improvement. Patience: 2/5\n\nEpoch 20/50\n----------------------------------------------------------------------\n\nEpoch 20 Summary:\n  Train Loss: 0.4120 | Train Corr: 0.5883\n  Val Loss:   0.3724 | Val Corr:   0.6340\n  >>> New best model saved! (Val Corr: 0.6340)\n\nEpoch 21/50\n----------------------------------------------------------------------\n\nEpoch 21 Summary:\n  Train Loss: 0.4149 | Train Corr: 0.5877\n  Val Loss:   0.3779 | Val Corr:   0.6212\n  No improvement. Patience: 1/5\n\nEpoch 22/50\n----------------------------------------------------------------------\n\nEpoch 22 Summary:\n  Train Loss: 0.4162 | Train Corr: 0.5830\n  Val Loss:   0.4223 | Val Corr:   0.6207\n  No improvement. Patience: 2/5\n\nEpoch 23/50\n----------------------------------------------------------------------\n\nEpoch 23 Summary:\n  Train Loss: 0.4118 | Train Corr: 0.5875\n  Val Loss:   0.3698 | Val Corr:   0.6223\n  No improvement. Patience: 3/5\n\nEpoch 24/50\n----------------------------------------------------------------------\n\nEpoch 24 Summary:\n  Train Loss: 0.4090 | Train Corr: 0.5889\n  Val Loss:   0.4219 | Val Corr:   0.6271\n  No improvement. Patience: 4/5\n\nEpoch 25/50\n----------------------------------------------------------------------\n\nEpoch 25 Summary:\n  Train Loss: 0.4110 | Train Corr: 0.5898\n  Val Loss:   0.4066 | Val Corr:   0.6200\n  No improvement. Patience: 5/5\n\n======================================================================\nEarly stopping triggered after 25 epochs\n======================================================================\n\n\n======================================================================\nTraining Complete! Best Val Correlation: 0.6340\nTotal Training Time: 295.01 seconds (4.92 minutes)\n======================================================================\n\n\n======================================================================\nFINAL EVALUATION ON TEST SET\n======================================================================\n\nTest Loss:        0.3720\nTest Correlation: 0.6351\n\nResults saved to results/RBFOX1_final_results_bilstm.json\nLoss curve saved to results/RBFOX1_loss_curve_bilstm.png\nCorrelation curve saved to results/RBFOX1_correlation_curve_bilstm.png\n\n======================================================================\nALL DONE!\n======================================================================\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"##Analysis\n\"\"\"\nAnalysis script to evaluate validation correlation vs test correlation.\n\nThis helps identify:\n- Overfitting (val corr much higher than test)\n- Underfitting (val corr plateaus far from test)\n- Optimal stopping point (where val corr is closest to test)\n\"\"\"\n\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom typing import Dict, Tuple\n\n\ndef load_results(protein_name: str, results_dir: str = \"/kaggle/working/results\") -> Dict:\n    \"\"\"Load training results from JSON.\"\"\"\n    results_path = os.path.join(results_dir, f\"{protein_name}_final_results.json\")\n    with open(results_path, 'r') as f:\n        return json.load(f)\n\n\ndef analyze_val_vs_test(results: Dict) -> Dict:\n    \"\"\"\n    Analyze how close validation correlation gets to test correlation.\n    \n    Returns:\n        Dictionary with analysis metrics\n    \"\"\"\n    history = results['history']\n    val_corrs = np.array(history['val_corr'])\n    test_corr = results['test_correlation']\n    \n    # Find best validation epoch\n    best_epoch = np.argmax(val_corrs)\n    best_val_corr = val_corrs[best_epoch]\n    \n    # Calculate differences\n    final_val_corr = val_corrs[-1]\n    best_gap = test_corr - best_val_corr\n    final_gap = test_corr - final_val_corr\n    \n    # Find epoch where val corr is closest to test\n    abs_diffs = np.abs(val_corrs - test_corr)\n    closest_epoch = np.argmin(abs_diffs)\n    closest_val_corr = val_corrs[closest_epoch]\n    min_gap = abs_diffs[closest_epoch]\n    \n    # Check for overfitting indicators\n    val_peak = np.max(val_corrs)\n    val_final = val_corrs[-1]\n    overfitting_drop = val_peak - val_final\n    \n    analysis = {\n        'test_correlation': test_corr,\n        'best_val_correlation': best_val_corr,\n        'best_val_epoch': int(best_epoch + 1),\n        'final_val_correlation': final_val_corr,\n        'final_epoch': len(val_corrs),\n        'closest_val_correlation': closest_val_corr,\n        'closest_epoch': int(closest_epoch + 1),\n        'best_gap': best_gap,\n        'final_gap': final_gap,\n        'min_gap': min_gap,\n        'overfitting_drop': overfitting_drop,\n        'relative_error_best': abs(best_gap / test_corr) * 100,\n        'relative_error_final': abs(final_gap / test_corr) * 100,\n        'relative_error_closest': (min_gap / test_corr) * 100\n    }\n    \n    return analysis\n\n\ndef print_analysis(analysis: Dict, protein_name: str):\n    \"\"\"Print formatted analysis report.\"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"VALIDATION vs TEST CORRELATION ANALYSIS - {protein_name}\")\n    print(f\"{'='*70}\\n\")\n    \n    print(\"Test Set Performance:\")\n    print(f\"  Test Correlation: {analysis['test_correlation']:.4f}\")\n    print()\n    \n    print(\"Validation Set Performance:\")\n    print(f\"  Best Val Correlation:  {analysis['best_val_correlation']:.4f} (Epoch {analysis['best_val_epoch']})\")\n    print(f\"  Final Val Correlation: {analysis['final_val_correlation']:.4f} (Epoch {analysis['final_epoch']})\")\n    print(f\"  Closest to Test:       {analysis['closest_val_correlation']:.4f} (Epoch {analysis['closest_epoch']})\")\n    print()\n    \n    print(\"Gap Analysis (Val - Test):\")\n    print(f\"  Gap at Best Val:    {analysis['best_gap']:+.4f} ({analysis['relative_error_best']:.2f}% relative error)\")\n    print(f\"  Gap at Final Epoch: {analysis['final_gap']:+.4f} ({analysis['relative_error_final']:.2f}% relative error)\")\n    print(f\"  Minimum Gap:        {analysis['min_gap']:+.4f} ({analysis['relative_error_closest']:.2f}% relative error)\")\n    print()\n    \n    print(\"Overfitting Indicators:\")\n    print(f\"  Val Peak-to-Final Drop: {analysis['overfitting_drop']:.4f}\")\n    if analysis['overfitting_drop'] > 0.01:\n        print(\"   Warning: Significant drop suggests overfitting\")\n    elif analysis['overfitting_drop'] < 0:\n        print(\"  Still improving at end of training\")\n    else:\n        print(\" Minimal overfitting\")\n    print()\n    \n    print(\"Interpretation:\")\n    if abs(analysis['best_gap']) < 0.01:\n        print(\"Excellent: Val correlation very close to test performance\")\n    elif abs(analysis['best_gap']) < 0.03:\n        print(\"Good: Val correlation reasonably predicts test performance\")\n    elif analysis['best_gap'] < 0:\n        print(\"Val correlation exceeds test (possible overfitting to val set)\")\n    else:\n        print(\"Val correlation underestimates test (consider more training)\")\n    \n    print(f\"\\n{'='*70}\\n\")\n\n\ndef plot_correlation_comparison(results: Dict, analysis: Dict, protein_name: str, \n                                 output_dir: str = \"results\"):\n    \"\"\"Create detailed plot comparing val and test correlations.\"\"\"\n    history = results['history']\n    epochs = np.arange(1, len(history['val_corr']) + 1)\n    val_corrs = np.array(history['val_corr'])\n    train_corrs = np.array(history['train_corr'])\n    test_corr = results['test_correlation']\n    \n    plt.figure(figsize=(12, 6))\n    \n    # Main plot\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_corrs, 'b-', label='Train', alpha=0.7, linewidth=2)\n    plt.plot(epochs, val_corrs, 'g-', label='Validation', alpha=0.7, linewidth=2)\n    plt.axhline(y=test_corr, color='r', linestyle='--', label='Test (final)', linewidth=2)\n    \n    # Mark best val epoch\n    best_epoch = analysis['best_val_epoch']\n    best_val = analysis['best_val_correlation']\n    plt.scatter([best_epoch], [best_val], color='green', s=100, zorder=5, \n                marker='o', edgecolors='black', linewidths=2)\n    \n    # Mark closest to test\n    closest_epoch = analysis['closest_epoch']\n    closest_val = analysis['closest_val_correlation']\n    plt.scatter([closest_epoch], [closest_val], color='orange', s=100, zorder=5,\n                marker='*', edgecolors='black', linewidths=2)\n    \n    plt.xlabel('Epoch', fontsize=11)\n    plt.ylabel('Spearman Correlation', fontsize=11)\n    plt.title(f'{protein_name}: Correlation Progression', fontsize=12, fontweight='bold')\n    plt.legend(loc='lower right')\n    plt.grid(True, alpha=0.3)\n    plt.ylim([0.35, max(train_corrs.max(), test_corr) + 0.05])\n    \n    # Gap plot\n    plt.subplot(1, 2, 2)\n    gaps = val_corrs - test_corr\n    plt.plot(epochs, gaps, 'purple', linewidth=2, label='Val - Test Gap')\n    plt.axhline(y=0, color='red', linestyle='--', linewidth=1.5, label='Perfect Match')\n    plt.fill_between(epochs, gaps, 0, where=(gaps >= 0), alpha=0.3, color='green', label='Val > Test')\n    plt.fill_between(epochs, gaps, 0, where=(gaps < 0), alpha=0.3, color='red', label='Val < Test')\n    \n    # Mark minimum gap\n    plt.scatter([closest_epoch], [gaps[closest_epoch-1]], color='orange', s=100, \n                zorder=5, marker='*', edgecolors='black', linewidths=2)\n    \n    plt.xlabel('Epoch', fontsize=11)\n    plt.ylabel('Correlation Gap (Val - Test)', fontsize=11)\n    plt.title('Validation-Test Gap Analysis', fontsize=12, fontweight='bold')\n    plt.legend(loc='best')\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    \n    output_path = os.path.join(output_dir, f\"{protein_name}_val_test_analysis.png\")\n    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n    print(f\"Analysis plot saved to {output_path}\")\n    plt.close()\n\n\ndef create_summary_table(analysis: Dict, output_dir: str = \"results\", \n                         protein_name: str = \"RBFOX1\"):\n    \"\"\"Create a summary table in JSON format.\"\"\"\n    summary = {\n        'protein': protein_name,\n        'test_correlation': round(analysis['test_correlation'], 4),\n        'validation_metrics': {\n            'best': {\n                'correlation': round(analysis['best_val_correlation'], 4),\n                'epoch': analysis['best_val_epoch'],\n                'gap_from_test': round(analysis['best_gap'], 4),\n                'relative_error_pct': round(analysis['relative_error_best'], 2)\n            },\n            'final': {\n                'correlation': round(analysis['final_val_correlation'], 4),\n                'epoch': analysis['final_epoch'],\n                'gap_from_test': round(analysis['final_gap'], 4),\n                'relative_error_pct': round(analysis['relative_error_final'], 2)\n            },\n            'closest_to_test': {\n                'correlation': round(analysis['closest_val_correlation'], 4),\n                'epoch': analysis['closest_epoch'],\n                'gap_from_test': round(analysis['min_gap'], 4),\n                'relative_error_pct': round(analysis['relative_error_closest'], 2)\n            }\n        },\n        'overfitting_indicator': round(analysis['overfitting_drop'], 4)\n    }\n    \n    output_path = os.path.join(output_dir, f\"{protein_name}_val_test_summary.json\")\n    with open(output_path, 'w') as f:\n        json.dump(summary, f, indent=2)\n    print(f\"Summary table saved to {output_path}\")\n\n\ndef main():\n    \"\"\"Run complete validation vs test analysis.\"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description=\"Analyze validation vs test correlation\")\n    parser.add_argument(\"--protein\", default=\"RBFOX1\", help=\"Protein name to analyze\")\n    parser.add_argument(\"--results-dir\", default=\"results\", help=\"Results directory\")\n    \n    args, _ = parser.parse_known_args()\n    \n    # Load results\n    print(f\"Loading results for {args.protein}...\")\n    results = load_results(args.protein, args.results_dir)\n    \n    # Analyze\n    analysis = analyze_val_vs_test(results)\n    \n    # Print report\n    print_analysis(analysis, args.protein)\n    \n    # Create visualizations\n    plot_correlation_comparison(results, analysis, args.protein, args.results_dir)\n    \n    # Create summary table\n    create_summary_table(analysis, args.results_dir, args.protein)\n    \n    print(\"Analysis complete!\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#MAIN\n\"\"\"\nRNA Binding Protein (RBP) Prediction using Convolutional Neural Networks\n\nThis script implements a CNN model for predicting RNA-protein binding affinities.\nThe CNN acts as a motif detector, identifying local sequence patterns associated \nwith high binding affinity, similar to how RBPs scan RNA for binding sites.\n\nArchitecture:\n    1. Convolutional Layer: Detects local motifs (binding patterns)\n    2. Pooling Layer: Aggregates motif information (configurable strategy)\n    3. Fully Connected Layers: Maps motif features to binding intensity\n    4. Dropout: Regularization to prevent overfitting\n\nKey Features:\n    - Multiple pooling strategies (global max, local max, none)\n    - Hyperparameter tuning for optimal performance\n    - Masked loss to handle missing data points\n    - Spearman correlation for evaluation\n\"\"\"\n\nimport argparse\nimport json\nimport os\nimport time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom typing import Dict, List, Tuple, Optional\nfrom argparse import Namespace\n\n# from utils import (\n#     RNACompeteLoader,\n#     configure_seed, \n#     masked_mse_loss, \n#     masked_spearman_correlation,\n#     plot,\n#     dataset_summary,\n# )\n# from config import RNAConfig\n\n\n# ============================================================================\n# Define the CNN Model Architecture\n# ============================================================================\n\nclass RNABindingCNN(nn.Module):\n    \"\"\"\n    Convolutional Neural Network for RNA Binding Prediction.\n    \n    The model uses 1D convolutions to detect sequence motifs, followed by\n    pooling to aggregate information, and fully connected layers for prediction.\n    \n    Args:\n        num_filters: Number of convolutional filters (motif detectors)\n        kernel_size: Size of each filter (motif length to detect)\n        dropout_rate: Dropout probability for regularization\n        input_channels: Number of input channels (4 for one-hot RNA)\n        seq_length: Length of input sequence (41 for RNAcompete)\n    \"\"\"\n    \n    def __init__(\n        self, \n        num_filters: int = 96,\n        kernel_size: int = 8,\n        dropout_rate: float = 0.5,\n        input_channels: int = 4,\n        seq_length: int = 41\n    ):\n        super(RNABindingCNN, self).__init__()\n        self.seq_length = seq_length\n        \n        # Convolutional Layer: Each filter acts as a motif detector\n        # Input: (batch, 4, 41) -> Output: (batch, num_filters, L_out)\n        self.conv1 = nn.Conv1d(\n            in_channels=input_channels,\n            out_channels=num_filters,\n            kernel_size=kernel_size,\n            stride=1,\n            padding=(kernel_size - 1) // 2  # Keep sequence length similar\n        )\n        \n        # Batch Normalization after convolution for better regularization\n        self.bn1 = nn.BatchNorm1d(num_filters)\n        \n        # Dropout after convolution for stronger regularization\n        self.dropout_conv = nn.Dropout(dropout_rate * 0.5)  # Lighter dropout for conv layer\n        \n        # Pooling Layer: Aggregates motif information\n        self.pool_size = 2\n        self.pool = nn.MaxPool1d(kernel_size=self.pool_size)\n\n        # Calculate the output length after convolution and pooling\n        conv_out_len = seq_length\n        pool_out_len = conv_out_len // self.pool_size\n        fc_input_size = num_filters * pool_out_len\n        \n        # Fully Connected Layers: Map motif features to binding intensity\n        self.fc1 = nn.Linear(fc_input_size, 128)\n        self.bn_fc1 = nn.BatchNorm1d(128)\n        \n        self.fc2 = nn.Linear(128, 64)\n        self.bn_fc2 = nn.BatchNorm1d(64)\n        \n        self.fc3 = nn.Linear(64, 1)  # Output: single binding intensity value\n        \n        self.dropout = nn.Dropout(dropout_rate)\n        \n    def forward(self, x):\n        \"\"\"\n        Forward pass through the network.\n        \n        Args:\n            x: Input tensor of shape (batch_size, seq_length, 4)\n        \n        Returns:\n            predictions: Tensor of shape (batch_size, 1)\n        \"\"\"\n        # Input shape: (batch, 41, 4)\n        # Conv1d expects: (batch, channels, length)\n        x = x.transpose(1, 2)  # -> (batch, 4, 41)\n        \n        # Apply Conv -> BatchNorm -> ReLU -> Dropout\n        x = self.conv1(x)  # -> (batch, num_filters, ~41)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = self.dropout_conv(x)\n        \n        # Local max: Preserve some positional information\n        x = self.pool(x)  # -> (batch, num_filters, ~20)\n        x = torch.flatten(x, 1)  # Flatten\n\n        # Fully connected layers with BatchNorm, ReLU and dropout\n        x = self.fc1(x)\n        x = self.bn_fc1(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        \n        x = self.fc2(x)\n        x = self.bn_fc2(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        \n        x = self.fc3(x)  # -> (batch, 1)\n        \n        return x\n\n# ============================================================================\n# Training Function\n# ============================================================================\n\ndef train_epoch(\n    model: nn.Module,\n    train_loader: DataLoader,\n    optimizer: optim.Optimizer,\n    device: torch.device\n) -> Tuple[float, float]:\n    \"\"\"\n    Train the model for one epoch.\n    \n    Args:\n        model: The CNN model\n        train_loader: DataLoader for training data\n        optimizer: Optimizer for updating weights\n        device: Device to run on (CPU or CUDA)\n    \n    Returns:\n        avg_loss: Average training loss\n        avg_corr: Average Spearman correlation\n    \"\"\"\n    model.train()\n    total_loss = 0.0\n    total_corr = 0.0\n    num_batches = 0\n    \n    for batch_idx, (sequences, targets, masks) in enumerate(train_loader):\n        # Move data to device\n        sequences = sequences.to(device)\n        targets = targets.to(device)\n        masks = masks.to(device)\n        \n        # Zero gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        predictions = model(sequences)\n        \n        # Compute masked loss (ignores NaN values)\n        loss = masked_mse_loss(predictions, targets, masks)\n        \n        # Backward pass\n        loss.backward()\n        \n        # Update weights\n        optimizer.step()\n        \n        # Compute metrics\n        with torch.no_grad():\n            corr = masked_spearman_correlation(predictions, targets, masks)\n        \n        total_loss += loss.item()\n        total_corr += corr.item()\n        num_batches += 1\n    \n    avg_loss = total_loss / num_batches\n    avg_corr = total_corr / num_batches\n    \n    return avg_loss, avg_corr\n\n\n# ============================================================================\n# Evaluation Function\n# ============================================================================\n\ndef evaluate(\n    model: nn.Module,\n    data_loader: DataLoader,\n    device: torch.device\n) -> Tuple[float, float]:\n    \"\"\"\n    Evaluate the model on validation or test set.\n    \n    Args:\n        model: The CNN model\n        data_loader: DataLoader for evaluation data\n        device: Device to run on\n    \n    Returns:\n        avg_loss: Average loss\n        avg_corr: Average Spearman correlation\n    \"\"\"\n    model.eval()\n    total_loss = 0.0\n    total_corr = 0.0\n    num_batches = 0\n    \n    with torch.no_grad():\n        for sequences, targets, masks in data_loader:\n            # Move data to device\n            sequences = sequences.to(device)\n            targets = targets.to(device)\n            masks = masks.to(device)\n            \n            # Forward pass\n            predictions = model(sequences)\n            \n            # Compute metrics\n            loss = masked_mse_loss(predictions, targets, masks)\n            corr = masked_spearman_correlation(predictions, targets, masks)\n            \n            total_loss += loss.item()\n            total_corr += corr.item()\n            num_batches += 1\n    \n    avg_loss = total_loss / num_batches\n    avg_corr = total_corr / num_batches\n    \n    return avg_loss, avg_corr\n\n\n# ============================================================================\n# Full Training Loop\n# ============================================================================\n\ndef train_model(\n    model: nn.Module,\n    train_loader: DataLoader,\n    val_loader: DataLoader,\n    num_epochs: int,\n    learning_rate: float,\n    device: torch.device,\n    save_path: Optional[str] = None,\n    weight_decay: float = 1e-3,\n    early_stopping_patience: int = 5\n) -> Dict[str, List[float]]:\n    \"\"\"\n    Train the model for multiple epochs with validation.\n    \n    Args:\n        model: The CNN model\n        train_loader: DataLoader for training data\n        val_loader: DataLoader for validation data\n        num_epochs: Number of training epochs\n        learning_rate: Learning rate for optimizer\n        device: Device to run on\n        save_path: Path to save the best model (optional)\n    \n    Returns:\n        history: Dictionary containing training metrics\n    \"\"\"\n    start_time = time.time()\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"Starting Training on {device}\")\n    print(f\"{'='*70}\")\n    print(f\"Regularization: Weight Decay = {weight_decay}, Early Stopping Patience = {early_stopping_patience}\")\n    \n    # Initialize optimizer with weight decay (L2 regularization)\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    \n    # Track metrics\n    history = {\n        'train_loss': [],\n        'train_corr': [],\n        'val_loss': [],\n        'val_corr': []\n    }\n    \n    best_val_corr = -1.0\n    patience_counter = 0\n    \n    # Training loop\n    for epoch in range(num_epochs):\n        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n        print(f\"{'-'*70}\")\n        \n\n        train_loss, train_corr = train_epoch(model, train_loader, optimizer, device)\n        \n\n        val_loss, val_corr = evaluate(model, val_loader, device)\n        \n\n        history['train_loss'].append(train_loss)\n        history['train_corr'].append(train_corr)\n        history['val_loss'].append(val_loss)\n        history['val_corr'].append(val_corr)\n        \n\n        print(f\"\\nEpoch {epoch + 1} Summary:\")\n        print(f\"  Train Loss: {train_loss:.4f} | Train Corr: {train_corr:.4f}\")\n        print(f\"  Val Loss:   {val_loss:.4f} | Val Corr:   {val_corr:.4f}\")\n        \n        # Save best model and check early stopping\n        if val_corr > best_val_corr:\n            best_val_corr = val_corr\n            patience_counter = 0  # Reset patience counter\n            if save_path:\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'val_corr': val_corr,\n                }, save_path)\n                print(f\"  >>> New best model saved! (Val Corr: {val_corr:.4f})\")\n        else:\n            patience_counter += 1\n            print(f\"  >>> No improvement for {patience_counter} epoch(s)\")\n            \n            if patience_counter >= early_stopping_patience:\n                print(f\"\\n  Early stopping triggered after {epoch + 1} epochs!\")\n                print(f\"  Best validation correlation: {best_val_corr:.4f}\")\n                break\n    \n    elapsed_time = time.time() - start_time\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"Training Complete! Best Val Correlation: {best_val_corr:.4f}\")\n    print(f\"Total Training Time: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n    print(f\"{'='*70}\\n\")\n    \n    history['elapsed_time'] = elapsed_time\n    \n    return history\n\n\n\n# ============================================================================\n# Hyperparameter Tuning\n# ============================================================================\n\ndef hyperparameter_search(\n    protein_name: str,\n    config: RNAConfig,\n    device: torch.device,\n    batch_size: int = 64,\n    num_epochs: int = 20\n) -> Tuple[Dict, Dict]:\n    \"\"\"\n    Perform grid search over hyperparameters.\n    \n    This function tests different combinations of:\n    - Filter sizes (32, 64, 128)\n    - Kernel sizes (6, 8, 10, 12)\n    - Dropout rates (0.2, 0.3, 0.5, 0.6)\n    - Learning rates (1e-3, 2e-4, 5e-4)\n    \n    Args:\n        protein_name: Name of the protein to train on\n        config: RNAConfig object\n        device: Device to run on\n        batch_size: Batch size for training\n        num_epochs: Number of epochs per configuration\n    \n    Returns:\n        best_config: Dictionary with best hyperparameters\n        all_results: Dictionary with all results\n    \"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"HYPERPARAMETER SEARCH FOR {protein_name}\")\n    print(f\"{'='*70}\\n\")\n    \n    # Define hyperparameter grid\n    param_grid = {\n        'num_filters': [32, 64, 96],\n        'kernel_size': [6, 8, 10, 12],\n        'dropout_rate': [0.2, 0.3, 0.5, 0.6],\n        'learning_rate': [1e-3, 2e-4, 5e-4]\n    }\n    \n    # Load data once - create loader instance that will be reused\n    print(\"Loading data (this will take a minute or two)...\")\n    loader = RNACompeteLoader(config)  # Create loader once\n    train_dataset = loader.get_data(protein_name, split='train')\n    val_dataset = loader.get_data(protein_name, split='val')\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    \n    # Generate all combinations (this could be large!)\n    # For practical purposes, we'll do a smaller random search\n    print(f\"\\nNote: Full grid search would test {np.prod([len(v) for v in param_grid.values()])} combinations.\")\n    print(\"Performing smart sampling of hyperparameter space...\\n\")\n    \n    # Sample a subset of combinations\n    all_results = []\n    best_val_corr = -1.0\n    best_config = None\n    \n    # 6 optimized variations around the best performer (Rank 1: 0.6016 correlation)\n    # Best: 64 filters, kernel 8, dropout 0.5, learning_rate 0.0005\n    test_configs = [\n        # Rank 1: Current best (keep for reproducibility)\n        {'num_filters': 96, 'kernel_size': 8, 'dropout_rate': 0.5, 'learning_rate': 5e-4},\n    \n        # Rank 2: Baseline reference\n        {'num_filters': 64, 'kernel_size': 8, 'dropout_rate': 0.5, 'learning_rate': 5e-4},\n    \n        # Variation 1: Higher capacity (test if still underfitting)\n        {'num_filters': 128, 'kernel_size': 8, 'dropout_rate': 0.5, 'learning_rate': 5e-4},\n    \n        # Variation 2: Slightly wider motif around best kernel\n        {'num_filters': 96, 'kernel_size': 10, 'dropout_rate': 0.5, 'learning_rate': 5e-4},\n    \n        # Variation 3: Slightly smaller kernel around best kernel\n        {'num_filters': 96, 'kernel_size': 6, 'dropout_rate': 0.5, 'learning_rate': 5e-4},\n    \n        # Variation 4: Capacity increase + mild regularization\n        {'num_filters': 128, 'kernel_size': 8, 'dropout_rate': 0.6, 'learning_rate': 5e-4},\n    ]\n\n    \n    total_configs = len(test_configs)\n    \n    for idx, params in enumerate(test_configs):\n        print(f\"\\n{'='*70}\")\n        print(f\"Configuration {idx + 1}/{total_configs}\")\n        print(f\"{'='*70}\")\n        print(f\"Parameters: {params}\")\n        \n        # Create model\n        model = RNABindingCNN(\n            num_filters=params['num_filters'],\n            kernel_size=params['kernel_size'],\n            dropout_rate=params['dropout_rate']\n        ).to(device)\n        \n        # Train model\n        history = train_model(\n            model=model,\n            train_loader=train_loader,\n            val_loader=val_loader,\n            num_epochs=num_epochs,\n            learning_rate=params['learning_rate'],\n            device=device,\n            save_path=None  # Don't save intermediate models\n        )\n        \n        # Get final validation correlation\n        final_val_corr = history['val_corr'][-1]\n        \n        # Store results\n        result = {\n            'params': params,\n            'final_val_corr': final_val_corr,\n            'history': history\n        }\n        all_results.append(result)\n        \n        # Update best\n        if final_val_corr > best_val_corr:\n            best_val_corr = final_val_corr\n            best_config = params.copy()\n            print(f\"\\n>>> NEW BEST CONFIGURATION! Val Corr: {final_val_corr:.4f}\")\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"HYPERPARAMETER SEARCH COMPLETE\")\n    print(f\"{'='*70}\")\n    print(f\"\\nBest Configuration:\")\n    for key, value in best_config.items():\n        print(f\"  {key}: {value}\")\n    print(f\"\\nBest Validation Correlation: {best_val_corr:.4f}\")\n    print(f\"{'='*70}\\n\")\n    \n    return best_config, all_results\n\n\n# ============================================================================\n# Data Exploration Only (summary + plots)\n# ============================================================================\n\ndef run_data_exploration(\n    protein_name: str,\n    config: RNAConfig,\n    output_dir: str = \"results\",\n    splits: Tuple[str, ...] = (\"train\", \"val\", \"test\")\n) -> Dict[str, Dict]:\n    \"\"\"Generate dataset summaries and plots without training.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    loader = RNACompeteLoader(config)\n\n    summaries: Dict[str, Dict] = {}\n    for split in splits:\n        print(f\"Loading {protein_name} {split} split for exploration...\")\n        dataset = loader.get_data(protein_name, split=split)\n        summary = dataset_summary(dataset)\n        summaries[split] = summary\n\n        summary_path = os.path.join(output_dir, f\"{protein_name}_{split}_summary.json\")\n        with open(summary_path, \"w\") as f:\n            json.dump(summary, f, indent=2)\n        print(f\"  Saved summary to {summary_path}\")\n\n    return summaries\n\n\n# ============================================================================\n# Main Execution\n# ============================================================================\n\ndef main():\n    \"\"\"\n    Main execution function.\n    \n    This function:\n    1. Sets up the environment (seed, device)\n    2. Loads the data\n    3. Performs hyperparameter search (optional)\n    4. Trains the final model with best hyperparameters\n    5. Evaluates on test set\n    6. Saves results and plots\n    \"\"\"\n    # parser = argparse.ArgumentParser(description=\"RNAcompete CNN trainer and data explorer\")\n    # parser.add_argument(\"--protein\", default=\"RBFOX1\", help=\"Protein name to process\")\n    # parser.add_argument(\"--batch-size\", type=int, default=64, help=\"Batch size for training\")\n    # parser.add_argument(\"--epochs\", type=int, default=30, help=\"Number of training epochs\")\n    # parser.add_argument(\"--hyperparam-search\", action=\"store_true\", help=\"Run hyperparameter search instead of default config\")\n    # parser.add_argument(\"--explore-only\", action=\"store_true\", help=\"Only run data summary/plots and skip training\")\n    # args = parser.parse_args()\n    args = Namespace(\n        protein=\"RBFOX1\",\n        batch_size=64,\n        epochs=50,\n        hyperparam_search=False,\n        explore_only=False,\n    )\n    # ========================================================================\n    # Configuration\n    # ========================================================================\n\n    configure_seed(42)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n\n    config = RNAConfig()\n    os.makedirs('results', exist_ok=True)\n    os.makedirs('models', exist_ok=True)\n\n    # Early exit: exploration-only mode\n    if args.explore_only:\n        print(f\"\\n{'='*70}\")\n        print(f\"DATA EXPLORATION FOR {args.protein}\")\n        print(f\"{'='*70}\\n\")\n        run_data_exploration(protein_name=args.protein, config=config, output_dir=\"results\")\n        print(\"Data exploration finished. Skipping training as requested.\")\n        return\n\n    # ========================================================================\n    # Hyperparameter Search\n    # ========================================================================\n\n    if args.hyperparam_search:\n        best_config, all_results = hyperparameter_search(\n            protein_name=args.protein,\n            config=config,\n            device=device,\n            batch_size=args.batch_size,\n            num_epochs=20  # Fewer epochs for search\n        )\n\n        results_file = f'results/{args.protein}_hyperparameter_search.json'\n        with open(results_file, 'w') as f:\n            serializable_results = []\n            for result in all_results:\n                serializable_results.append({\n                    'params': result['params'],\n                    'final_val_corr': result['final_val_corr']\n                })\n            json.dump({\n                'best_config': best_config,\n                'all_results': serializable_results\n            }, f, indent=2)\n        print(f\"Hyperparameter search results saved to {results_file}\")\n\n    else:\n        best_config = {\n            'num_filters': 96,\n            'kernel_size': 8,\n            'dropout_rate': 0.6,\n            'learning_rate': 5e-4\n        }\n        print(f\"\\nUsing default configuration:\")\n        for key, value in best_config.items():\n            print(f\"  {key}: {value}\")\n\n    # ========================================================================\n    # Train Final Model with Best/Default Configuration\n    # ========================================================================\n\n    print(f\"\\n{'='*70}\")\n    print(f\"TRAINING FINAL MODEL FOR {args.protein}\")\n    print(f\"{'='*70}\\n\")\n\n    print(\"Loading data (this will take a minute or two)...\")\n    loader = RNACompeteLoader(config)\n    train_dataset = loader.get_data(args.protein, split='train')\n    val_dataset = loader.get_data(args.protein, split='val')\n    test_dataset = loader.get_data(args.protein, split='test')\n\n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n\n    print(f\"Train set: {len(train_dataset)} samples\")\n    print(f\"Val set:   {len(val_dataset)} samples\")\n    print(f\"Test set:  {len(test_dataset)} samples\")\n\n    model = RNABindingCNN(\n        num_filters=best_config['num_filters'],\n        kernel_size=best_config['kernel_size'],\n        dropout_rate=best_config['dropout_rate']\n    ).to(device)\n\n    print(f\"\\nModel Architecture:\")\n    print(model)\n    print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n    model_save_path = f\"models/{args.protein}_best_model.pt\"\n    history = train_model(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        num_epochs=args.epochs,\n        learning_rate=best_config['learning_rate'],\n        device=device,\n        save_path=model_save_path\n    )\n\n    print(f\"\\n{'='*70}\")\n    print(f\"FINAL EVALUATION ON TEST SET\")\n    print(f\"{'='*70}\\n\")\n\n    checkpoint = torch.load(model_save_path, weights_only=True)\n    model.load_state_dict(checkpoint['model_state_dict'])\n\n    test_loss, test_corr = evaluate(model, test_loader, device)\n\n    print(f\"Test Loss:        {test_loss:.4f}\")\n    print(f\"Test Correlation: {test_corr:.4f}\")\n\n    final_results = {\n        'protein': args.protein,\n        'config': best_config,\n        'test_loss': test_loss,\n        'test_correlation': test_corr,\n        'training_time_seconds': history['elapsed_time'],\n        'training_time_minutes': history['elapsed_time'] / 60,\n        # Serialize lists to floats; scalars remain floats\n        'history': {k: ([float(v) for v in vals] if isinstance(vals, list) else float(vals)) for k, vals in history.items()}\n    }\n\n    results_file = f'results/{args.protein}_final_results.json'\n    with open(results_file, 'w') as f:\n        json.dump(final_results, f, indent=2)\n    print(f\"\\nResults saved to {results_file}\")\n\n    # Use actual number of epochs from history (handles early stopping)\n    actual_epochs = len(history['train_loss'])\n    epochs = list(range(1, actual_epochs + 1))\n\n    plot(\n        epochs=epochs,\n        plottables={\n            'Train Loss': history['train_loss'],\n            'Val Loss': history['val_loss']\n        },\n        filename=f'results/{args.protein}_loss_curve.png'\n    )\n    print(f\"Loss curve saved to results/{args.protein}_loss_curve.png\")\n\n    plot(\n        epochs=epochs,\n        plottables={\n            'Train Correlation': history['train_corr'],\n            'Val Correlation': history['val_corr']\n        },\n        filename=f'results/{args.protein}_correlation_curve.png',\n        ylim=[0, 1]\n    )\n    print(f\"Correlation curve saved to results/{args.protein}_correlation_curve.png\")\n\n    print(f\"\\n{'='*70}\")\n    print(f\"ALL DONE!\")\n    print(f\"{'='*70}\\n\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"scrolled":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2026-01-08T21:56:36.239644Z","iopub.execute_input":"2026-01-08T21:56:36.239919Z","iopub.status.idle":"2026-01-08T22:00:32.614617Z","shell.execute_reply.started":"2026-01-08T21:56:36.239898Z","shell.execute_reply":"2026-01-08T22:00:32.614078Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\nUsing default configuration:\n  num_filters: 96\n  kernel_size: 8\n  dropout_rate: 0.6\n  learning_rate: 0.0005\n\n======================================================================\nTRAINING FINAL MODEL FOR RBFOX1\n======================================================================\n\nLoading data (this will take a minute or two)...\nFound preprocessed data at /kaggle/input/rnacompete/RBFOX1_train_data.pt. Loading...\nFound preprocessed data at /kaggle/input/rnacompete/RBFOX1_val_data.pt. Loading...\nFound preprocessed data at /kaggle/input/rnacompete/RBFOX1_test_data.pt. Loading...\nTrain set: 96261 samples\nVal set:   24065 samples\nTest set:  121031 samples\n\nModel Architecture:\nRNABindingCNN(\n  (conv1): Conv1d(4, 96, kernel_size=(8,), stride=(1,), padding=(3,))\n  (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout_conv): Dropout(p=0.3, inplace=False)\n  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=1920, out_features=128, bias=True)\n  (bn_fc1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc2): Linear(in_features=128, out_features=64, bias=True)\n  (bn_fc2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc3): Linear(in_features=64, out_features=1, bias=True)\n  (dropout): Dropout(p=0.6, inplace=False)\n)\n\nTotal parameters: 257,953\n\n======================================================================\nStarting Training on cuda\n======================================================================\nRegularization: Weight Decay = 0.001, Early Stopping Patience = 5\n\nEpoch 1/50\n----------------------------------------------------------------------\n\nEpoch 1 Summary:\n  Train Loss: 0.9622 | Train Corr: 0.2458\n  Val Loss:   0.7864 | Val Corr:   0.4403\n  >>> New best model saved! (Val Corr: 0.4403)\n\nEpoch 2/50\n----------------------------------------------------------------------\n\nEpoch 2 Summary:\n  Train Loss: 0.8029 | Train Corr: 0.4056\n  Val Loss:   0.7254 | Val Corr:   0.4702\n  >>> New best model saved! (Val Corr: 0.4702)\n\nEpoch 3/50\n----------------------------------------------------------------------\n\nEpoch 3 Summary:\n  Train Loss: 0.7549 | Train Corr: 0.4387\n  Val Loss:   0.6888 | Val Corr:   0.4780\n  >>> New best model saved! (Val Corr: 0.4780)\n\nEpoch 4/50\n----------------------------------------------------------------------\n\nEpoch 4 Summary:\n  Train Loss: 0.7195 | Train Corr: 0.4574\n  Val Loss:   0.6302 | Val Corr:   0.5007\n  >>> New best model saved! (Val Corr: 0.5007)\n\nEpoch 5/50\n----------------------------------------------------------------------\n\nEpoch 5 Summary:\n  Train Loss: 0.6897 | Train Corr: 0.4685\n  Val Loss:   0.5862 | Val Corr:   0.5212\n  >>> New best model saved! (Val Corr: 0.5212)\n\nEpoch 6/50\n----------------------------------------------------------------------\n\nEpoch 6 Summary:\n  Train Loss: 0.6556 | Train Corr: 0.4800\n  Val Loss:   0.5671 | Val Corr:   0.5277\n  >>> New best model saved! (Val Corr: 0.5277)\n\nEpoch 7/50\n----------------------------------------------------------------------\n\nEpoch 7 Summary:\n  Train Loss: 0.6251 | Train Corr: 0.4897\n  Val Loss:   0.5176 | Val Corr:   0.5430\n  >>> New best model saved! (Val Corr: 0.5430)\n\nEpoch 8/50\n----------------------------------------------------------------------\n\nEpoch 8 Summary:\n  Train Loss: 0.5995 | Train Corr: 0.4936\n  Val Loss:   0.4954 | Val Corr:   0.5491\n  >>> New best model saved! (Val Corr: 0.5491)\n\nEpoch 9/50\n----------------------------------------------------------------------\n\nEpoch 9 Summary:\n  Train Loss: 0.5776 | Train Corr: 0.5013\n  Val Loss:   0.4903 | Val Corr:   0.5635\n  >>> New best model saved! (Val Corr: 0.5635)\n\nEpoch 10/50\n----------------------------------------------------------------------\n\nEpoch 10 Summary:\n  Train Loss: 0.5646 | Train Corr: 0.5091\n  Val Loss:   0.4569 | Val Corr:   0.5655\n  >>> New best model saved! (Val Corr: 0.5655)\n\nEpoch 11/50\n----------------------------------------------------------------------\n\nEpoch 11 Summary:\n  Train Loss: 0.5475 | Train Corr: 0.5109\n  Val Loss:   0.4989 | Val Corr:   0.5767\n  >>> New best model saved! (Val Corr: 0.5767)\n\nEpoch 12/50\n----------------------------------------------------------------------\n\nEpoch 12 Summary:\n  Train Loss: 0.5308 | Train Corr: 0.5194\n  Val Loss:   0.4558 | Val Corr:   0.5754\n  >>> No improvement for 1 epoch(s)\n\nEpoch 13/50\n----------------------------------------------------------------------\n\nEpoch 13 Summary:\n  Train Loss: 0.5166 | Train Corr: 0.5233\n  Val Loss:   0.4308 | Val Corr:   0.5820\n  >>> New best model saved! (Val Corr: 0.5820)\n\nEpoch 14/50\n----------------------------------------------------------------------\n\nEpoch 14 Summary:\n  Train Loss: 0.5135 | Train Corr: 0.5240\n  Val Loss:   0.4314 | Val Corr:   0.5826\n  >>> New best model saved! (Val Corr: 0.5826)\n\nEpoch 15/50\n----------------------------------------------------------------------\n\nEpoch 15 Summary:\n  Train Loss: 0.5003 | Train Corr: 0.5291\n  Val Loss:   0.4191 | Val Corr:   0.5915\n  >>> New best model saved! (Val Corr: 0.5915)\n\nEpoch 16/50\n----------------------------------------------------------------------\n\nEpoch 16 Summary:\n  Train Loss: 0.4921 | Train Corr: 0.5309\n  Val Loss:   0.4339 | Val Corr:   0.5890\n  >>> No improvement for 1 epoch(s)\n\nEpoch 17/50\n----------------------------------------------------------------------\n\nEpoch 17 Summary:\n  Train Loss: 0.4868 | Train Corr: 0.5311\n  Val Loss:   0.4177 | Val Corr:   0.5943\n  >>> New best model saved! (Val Corr: 0.5943)\n\nEpoch 18/50\n----------------------------------------------------------------------\n\nEpoch 18 Summary:\n  Train Loss: 0.4810 | Train Corr: 0.5375\n  Val Loss:   0.4328 | Val Corr:   0.5971\n  >>> New best model saved! (Val Corr: 0.5971)\n\nEpoch 19/50\n----------------------------------------------------------------------\n\nEpoch 19 Summary:\n  Train Loss: 0.4853 | Train Corr: 0.5350\n  Val Loss:   0.4091 | Val Corr:   0.5975\n  >>> New best model saved! (Val Corr: 0.5975)\n\nEpoch 20/50\n----------------------------------------------------------------------\n\nEpoch 20 Summary:\n  Train Loss: 0.4763 | Train Corr: 0.5405\n  Val Loss:   0.4127 | Val Corr:   0.5969\n  >>> No improvement for 1 epoch(s)\n\nEpoch 21/50\n----------------------------------------------------------------------\n\nEpoch 21 Summary:\n  Train Loss: 0.4712 | Train Corr: 0.5373\n  Val Loss:   0.4223 | Val Corr:   0.5978\n  >>> New best model saved! (Val Corr: 0.5978)\n\nEpoch 22/50\n----------------------------------------------------------------------\n\nEpoch 22 Summary:\n  Train Loss: 0.4718 | Train Corr: 0.5387\n  Val Loss:   0.4066 | Val Corr:   0.6023\n  >>> New best model saved! (Val Corr: 0.6023)\n\nEpoch 23/50\n----------------------------------------------------------------------\n\nEpoch 23 Summary:\n  Train Loss: 0.4744 | Train Corr: 0.5420\n  Val Loss:   0.3991 | Val Corr:   0.6030\n  >>> New best model saved! (Val Corr: 0.6030)\n\nEpoch 24/50\n----------------------------------------------------------------------\n\nEpoch 24 Summary:\n  Train Loss: 0.4710 | Train Corr: 0.5441\n  Val Loss:   0.4049 | Val Corr:   0.6026\n  >>> No improvement for 1 epoch(s)\n\nEpoch 25/50\n----------------------------------------------------------------------\n\nEpoch 25 Summary:\n  Train Loss: 0.4663 | Train Corr: 0.5453\n  Val Loss:   0.4059 | Val Corr:   0.6072\n  >>> New best model saved! (Val Corr: 0.6072)\n\nEpoch 26/50\n----------------------------------------------------------------------\n\nEpoch 26 Summary:\n  Train Loss: 0.4747 | Train Corr: 0.5442\n  Val Loss:   0.4147 | Val Corr:   0.6067\n  >>> No improvement for 1 epoch(s)\n\nEpoch 27/50\n----------------------------------------------------------------------\n\nEpoch 27 Summary:\n  Train Loss: 0.4681 | Train Corr: 0.5463\n  Val Loss:   0.4165 | Val Corr:   0.6112\n  >>> New best model saved! (Val Corr: 0.6112)\n\nEpoch 28/50\n----------------------------------------------------------------------\n\nEpoch 28 Summary:\n  Train Loss: 0.4672 | Train Corr: 0.5426\n  Val Loss:   0.3987 | Val Corr:   0.6111\n  >>> No improvement for 1 epoch(s)\n\nEpoch 29/50\n----------------------------------------------------------------------\n\nEpoch 29 Summary:\n  Train Loss: 0.4602 | Train Corr: 0.5444\n  Val Loss:   0.4029 | Val Corr:   0.6064\n  >>> No improvement for 2 epoch(s)\n\nEpoch 30/50\n----------------------------------------------------------------------\n\nEpoch 30 Summary:\n  Train Loss: 0.4640 | Train Corr: 0.5459\n  Val Loss:   0.4228 | Val Corr:   0.6094\n  >>> No improvement for 3 epoch(s)\n\nEpoch 31/50\n----------------------------------------------------------------------\n\nEpoch 31 Summary:\n  Train Loss: 0.4619 | Train Corr: 0.5442\n  Val Loss:   0.4025 | Val Corr:   0.6090\n  >>> No improvement for 4 epoch(s)\n\nEpoch 32/50\n----------------------------------------------------------------------\n\nEpoch 32 Summary:\n  Train Loss: 0.4580 | Train Corr: 0.5453\n  Val Loss:   0.4176 | Val Corr:   0.6087\n  >>> No improvement for 5 epoch(s)\n\n  Early stopping triggered after 32 epochs!\n  Best validation correlation: 0.6112\n\n======================================================================\nTraining Complete! Best Val Correlation: 0.6112\nTotal Training Time: 231.91 seconds (3.87 minutes)\n======================================================================\n\n\n======================================================================\nFINAL EVALUATION ON TEST SET\n======================================================================\n\nTest Loss:        0.4197\nTest Correlation: 0.6125\n\nResults saved to results/RBFOX1_final_results.json\nLoss curve saved to results/RBFOX1_loss_curve.png\nCorrelation curve saved to results/RBFOX1_correlation_curve.png\n\n======================================================================\nALL DONE!\n======================================================================\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAiMAAAG2CAYAAACtaYbcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABKXklEQVR4nO3de1xUdd4H8M/MMBcGGO53UVAJNBUUldCtLCmy1tUum6uutzVdSy1jrbQtL/UUluVjpeVuF619Ms22smctfYzSysw7XkrwhoLKHbkNMAMz5/njDAMjoIwCP5DP+/Wa15lz5syc7xynzoff+Z3fUUiSJIGIiIhIEKXoAoiIiKhrYxghIiIioRhGiIiISCiGESIiIhKKYYSIiIiEYhghIiIioRhGiIiISCiGESIiIhKKYYSIiIiEYhghIiIioZwOIz/88ANGjx6NkJAQKBQKfPnll1d9z44dOzBo0CBotVr07t0b69atu4ZSiYiI6EbkdBgxGo2IiYnB6tWrW7R+ZmYm7rvvPtxxxx1IS0vDvHnz8Mgjj2Dbtm1OF0tEREQ3HsX13ChPoVDgiy++wNixY5td55lnnsGWLVtw7Ngx+7I//elPKCkpwdatW69100RERHSDcGnrDezevRuJiYkOy5KSkjBv3rxm32MymWAymezzVqsVxcXF8PX1hUKhaKtSiYiIqBVJkoTy8nKEhIRAqWz+ZEybh5Hc3FwEBgY6LAsMDERZWRmqqqrg6ura6D0pKSlYunRpW5dGRERE7SA7OxvdunVr9vU2DyPXYuHChUhOTrbPl5aWonv37sjOzobBYBBYGREREbVUWVkZwsLC4OHhccX12jyMBAUFIS8vz2FZXl4eDAZDk60iAKDVaqHVahstNxgMDCNERESdzNW6WLT5OCMJCQlITU11WLZ9+3YkJCS09aaJiIioE3A6jFRUVCAtLQ1paWkA5Et309LSkJWVBUA+xTJ58mT7+rNmzcKZM2fw9NNPIz09HW+//TY+/fRTPPnkk63zDYiIiKhTczqM7N+/HwMHDsTAgQMBAMnJyRg4cCAWLVoEAMjJybEHEwCIiIjAli1bsH37dsTExOD111/He++9h6SkpFb6CkRERNSZXdc4I+2lrKwMnp6eKC0tZZ8RIiIbi8WCmpoa0WVQF6ZWq6FSqZp9vaXH7w55NQ0RETVPkiTk5uaipKREdClE8PLyQlBQ0HWNA8YwQkTUydQFkYCAAOj1eg4GSUJIkoTKykrk5+cDAIKDg6/5sxhGiIg6EYvFYg8ivr6+osuhLq5uiI78/HwEBARc8ZTNlbT5pb1ERNR66vqI6PV6wZUQyep+i9fTf4lhhIioE+KpGeooWuO3yDBCREREQjGMEBFRpxQeHo6VK1eKLqPVnT17FgqFwj64qOjPaQ8MI0RE1KYUCsUVH0uWLLmmz923bx9mzpx53fWdOnUK06ZNQ7du3aDVahEREYHx48dj//791/3Z7WXq1KkYO3asw7KwsDDk5OSgX79+YopyAq+mISKiNpWTk2N/vnHjRixatAgZGRn2Ze7u7vbnkiTBYrHAxeXqhyd/f//rrm3//v0YOXIk+vXrh3/84x+Ijo5GeXk5Nm/ejL/97W/YuXPnNX2u2WyGRqNptLympgZqtfp6y24RlUqFoKCgdtnW9WLLCBERtamgoCD7w9PTEwqFwj6fnp4ODw8PfPPNN4iLi4NWq8VPP/2E06dPY8yYMQgMDIS7uzuGDBmCb7/91uFzLz9No1Ao8N577+H++++HXq9HZGQkvvrqq2brkiQJU6dORWRkJH788Ufcd9996NWrF2JjY7F48WJs3rzZvu7Ro0dx5513wtXVFb6+vpg5cyYqKirsr9e1TLz00ksICQlBVFSU/TTJxo0bcfvtt0On0+Hjjz8GALz33nvo06cPdDodoqOj8fbbbzdbp8ViwfTp0xEREQFXV1dERUXhjTfesL++ZMkSfPjhh9i8ebO9tWnHjh1NnqbZuXMnhg4dCq1Wi+DgYCxYsAC1tbX210eMGIHHH38cTz/9NHx8fBAUFHTNLVfOYMsIEVEnJ0kSqmos7b5dV7Wq1a7qWbBgAV577TX07NkT3t7eyM7Oxr333ouXXnoJWq0WH330EUaPHo2MjAx079692c9ZunQpXn31VSxfvhxvvfUWJk6ciHPnzsHHx6fRumlpafj111+xfv16KJWN/zb38vICABiNRiQlJSEhIQH79u1Dfn4+HnnkEcyZMwfr1q2zr5+amgqDwYDt27c3+m6vv/46Bg4caA8kixYtwqpVqzBw4EAcOnQIM2bMgJubG6ZMmdKoDqvVim7dumHTpk3w9fXFzz//jJkzZyI4OBgPP/ww5s+fj+PHj6OsrAxr164FAPj4+ODixYsOn3PhwgXce++9mDp1Kj766COkp6djxowZ0Ol0DoHjww8/RHJyMvbs2YPdu3dj6tSpGD58OO66665m9/v1YhghIurkqmos6LtoW7tv97cXkqDXtM5h5IUXXnA42Pn4+CAmJsY+/+KLL+KLL77AV199hTlz5jT7OVOnTsX48eMBAC+//DLefPNN7N27F/fcc0+jdU+ePAkAiI6OvmJt69evR3V1NT766CO4ubkBAFatWoXRo0fjlVdeQWBgIADAzc0N7733nv30zNmzZwEA8+bNwwMPPGD/vMWLF+P111+3L4uIiMBvv/2Gf/zjH02GEbVajaVLl9rnIyIisHv3bnz66ad4+OGH4e7uDldXV5hMpiuelnn77bcRFhaGVatWQaFQIDo6GhcvXsQzzzyDRYsW2QPZgAEDsHjxYgBAZGQkVq1ahdTUVIYRIiK6sQ0ePNhhvqKiAkuWLMGWLVuQk5OD2tpaVFVVOdwVvikDBgywP3dzc4PBYLAPV365lt4n9vjx44iJibEHEQAYPnw4rFYrMjIy7GGkf//+TfYTafjdjEYjTp8+jenTp2PGjBn25bW1tfD09Gy2htWrV+ODDz5AVlYWqqqqYDabERsb26L6G36PhIQEh9as4cOHo6KiAufPn7e3ODXch4A8zHtz+7C1MIwQEXVyrmoVfnshSch2W0vDAz0AzJ8/H9u3b8drr72G3r17w9XVFQ899BDMZvMVP+fyzqEKhQJWq7XJdW+66SYAQHp6OgYOHHgd1csu/w5NLa/rZ/Luu+8iPj7eYb3mhlLfsGED5s+fj9dffx0JCQnw8PDA8uXLsWfPnuuuuSnO7MPWwjBCRNTJKRSKVjtd0lHs2rULU6dOxf333w9APojXnfZoLbGxsejbty9ef/11jBs3rlG/kZKSEnh5eaFPnz5Yt24djEajPVjs2rULSqUSUVFRTm0zMDAQISEhOHPmDCZOnNii9+zatQvDhg3DY489Zl92+vRph3U0Gg0sliv3G+rTpw/+/e9/Q5Ike+vIrl274OHhgW7dujn1PVobr6YhIqIOJzIyEp9//jnS0tJw+PBhTJgwodX/OlcoFFi7di1OnDiBW2+9FV9//TXOnDmDI0eO4KWXXsKYMWMAABMnToROp8OUKVNw7NgxfP/995g7dy4mTZpkP0XjjKVLlyIlJQVvvvkmTpw4gaNHj2Lt2rVYsWJFk+tHRkZi//792LZtG06cOIHnn38e+/btc1gnPDwcR44cQUZGBgoLC5u8T8xjjz2G7OxszJ07F+np6di8eTMWL16M5OTkJjvwtieGESIi6nBWrFgBb29vDBs2DKNHj0ZSUhIGDRrU6tsZOnQo9u/fj969e2PGjBno06cP/vCHP+DXX3+1Xzas1+uxbds2FBcXY8iQIXjooYcwcuRIrFq16pq2+cgjj+C9997D2rVr0b9/f9x+++1Yt24dIiIimlz/r3/9Kx544AGMGzcO8fHxKCoqcmglAYAZM2YgKioKgwcPhr+/P3bt2tXoc0JDQ/H1119j7969iImJwaxZszB9+nQ899xz1/Q9WpNCamkPHoHKysrg6emJ0tJSGAwG0eUQEQlTXV2NzMxMREREQKfTiS6H6Iq/yZYev9kyQkREREIxjBAREZFQDCNEREQkFMMIERERCcUwQkREREIxjBAREZFQDCNEREQkFMMIERERCcUwQkREREIxjBARUacwYsQIzJs3T3QZTmutujvr928JhhEiImpTo0ePxj333NPkaz/++CMUCgWOHDnSKtsym8149dVXERMTA71eDz8/PwwfPhxr165t8uZxHdGOHTugUChQUlLisPzzzz/Hiy++KKaoNnZj3XOaiIg6nOnTp+PBBx/E+fPnG92qfu3atRg8eDAGDBhw3dsxm81ISkrC4cOH8eKLL2L48OEwGAz45Zdf8Nprr2HgwIGIjY11+nMlSYLFYoGLi+Mh02w2Q6PRXHfdLeXj49Nu22pvbBkhIqI29fvf/x7+/v5Yt26dw/KKigps2rQJ06dPR1FREcaPH4/Q0FDo9Xr0798fn3zyiVPbWblyJX744QekpqZi9uzZiI2NRc+ePTFhwgTs2bMHkZGRAACTyYTHH38cAQEB0Ol0+N3vfod9+/bZP6euZeKbb75BXFwctFotfvrpJ4wYMQJz5szBvHnz4Ofnh6SkJADAsWPHMGrUKLi7uyMwMBCTJk1CYWFhs3X+61//wuDBg+Hh4YGgoCBMmDAB+fn5AICzZ8/ijjvuAAB4e3tDoVBg6tSpABqfprl06RImT54Mb29v6PV6jBo1CidPnrS/vm7dOnh5eWHbtm3o06cP3N3dcc899yAnJ8ep/doeGEaIiDo7SQLMxvZ/tPCm7y4uLpg8eTLWrVuHhjeK37RpEywWC8aPH4/q6mrExcVhy5YtOHbsGGbOnIlJkyZh7969Ld4NH3/8MRITEzFw4MBGr6nVari5uQEAnn76afz73//Ghx9+iIMHD6J3795ISkpCcXGxw3sWLFiAZcuW4fjx4/aWmw8//BAajQa7du3CmjVrUFJSgjvvvBMDBw7E/v37sXXrVuTl5eHhhx9uts6amhq8+OKLOHz4ML788kucPXvWHjjCwsLw73//GwCQkZGBnJwcvPHGG01+ztSpU7F//3589dVX2L17NyRJwr333utwOqqyshKvvfYa/vWvf+GHH35AVlYW5s+f3+J92l54moaIqLOrqQReDmn/7T57EdC4tWjVv/zlL1i+fDl27tyJESNGAJBP0Tz44IPw9PSEp6enw0Fy7ty52LZtGz799FMMHTq0Rds4efKk/bObYzQa8c4772DdunUYNWoUAODdd9/F9u3b8f777+Opp56yr/vCCy/grrvucnh/ZGQkXn31Vfv8f/3Xf2HgwIF4+eWX7cs++OADhIWF4cSJE7jpppua3Bd1evbsiTfffBNDhgxBRUUF3N3d7adjAgIC4OXl1ex3/eqrr7Br1y4MGzYMgBzGwsLC8OWXX+KPf/wjADn4rFmzBr169QIAzJkzBy+88MIV95EIbBkhIqI2Fx0djWHDhuGDDz4AAJw6dQo//vgjpk+fDgCwWCx48cUX0b9/f/j4+MDd3R3btm1DVlZWi7chtaCl5vTp06ipqcHw4cPty9RqNYYOHYrjx487rDt48OBG74+Li3OYP3z4ML7//nu4u7vbH9HR0fZtNeXAgQMYPXo0unfvDg8PD9x+++0A4NR3PX78OFxcXBAfH29f5uvri6ioKIfvodfr7UEEAIKDg+2nhDoStowQEXV2ar3cSiFiu06YPn065s6di9WrV2Pt2rXo1auX/UC8fPlyvPHGG1i5ciX69+8PNzc3zJs3D2azucWff9NNNyE9Pd2pmq6k7rTOlZZVVFRg9OjReOWVVxqtGxwc3GiZ0WhEUlISkpKS8PHHH8Pf3x9ZWVlISkpy6ru2lFqtdphXKBQtCm3tjS0jRESdnUIhny5p74dC4VSZDz/8MJRKJdavX4+PPvoIf/nLX6CwfcauXbswZswY/PnPf0ZMTAx69uyJEydOOPX5EyZMwLfffotDhw41eq2mpgZGoxG9evWy9/lo+Nq+ffvQt29fp7YHAIMGDcKvv/6K8PBw9O7d2+HRVJhJT09HUVERli1bhltvvRXR0dGNWirqrtCxWCzNbrdPnz6ora3Fnj177MuKioqQkZFxTd9DNIYRIiJqF+7u7hg3bhwWLlyInJwce6dNQO6LsX37dvz88884fvw4/vrXvyIvL8+pz583bx6GDx+OkSNHYvXq1Th8+DDOnDmDTz/9FLfccgtOnjwJNzc3PProo3jqqaewdetW/Pbbb5gxYwYqKyvtp4ycMXv2bBQXF2P8+PHYt28fTp8+jW3btmHatGlNhonu3btDo9HgrbfewpkzZ/DVV181GjukR48eUCgU+M9//oOCggJUVFQ0+pzIyEiMGTMGM2bMwE8//YTDhw/jz3/+M0JDQzFmzBinv4doDCNERNRupk+fjkuXLiEpKQkhIfWdbp977jkMGjQISUlJGDFiBIKCgjB27FinPlur1WL79u14+umn8Y9//AO33HILhgwZgjfffBOPP/44+vXrBwBYtmwZHnzwQUyaNAmDBg3CqVOnsG3bNnh7ezv9fUJCQrBr1y5YLBbcfffd6N+/P+bNmwcvLy8olY0PsXWXOG/atAl9+/bFsmXL8NprrzmsExoaiqVLl2LBggUIDAzEnDlzmtz22rVrERcXh9///vdISEiAJEn4+uuvG52a6QwUUkc8eXSZsrIyeHp6orS0FAaDQXQ5RETCVFdXIzMzExEREdDpdKLLIbrib7Klx2+2jBAREZFQDCNEREQkFMMIERERCcUwQkREREIxjBARdUKd4NoD6iJa47fIMEJE1InUXbZZWVkpuBIiWd1v8XouKeZw8EREnYhKpYKXl5d91E69Xm8fxZSoPUmShMrKSuTn58PLywsqleqaP4thhIiokwkKCgKADnnDM+p6vLy87L/Ja8UwQkTUySgUCgQHByMgIAA1NTWiy6EuTK1WX1eLSB2GESKiTkqlUrXKgYBINHZgJSIiIqEYRoiIiEgohhEiIiISimGEiIiIhGIYISIiIqEYRoiIiEgohhEiIiISimGEiIiIhGIYISIiIqEYRoiIiEgohhEiIiISimGEiIiIhGIYISIiIqEYRoiIiEioawojq1evRnh4OHQ6HeLj47F3794rrr9y5UpERUXB1dUVYWFhePLJJ1FdXX1NBRMREdGNxekwsnHjRiQnJ2Px4sU4ePAgYmJikJSUhPz8/CbXX79+PRYsWIDFixfj+PHjeP/997Fx40Y8++yz1108ERERdX5Oh5EVK1ZgxowZmDZtGvr27Ys1a9ZAr9fjgw8+aHL9n3/+GcOHD8eECRMQHh6Ou+++G+PHj79qawoRERF1DU6FEbPZjAMHDiAxMbH+A5RKJCYmYvfu3U2+Z9iwYThw4IA9fJw5cwZff/017r333ma3YzKZUFZW5vAgIiKiG5OLMysXFhbCYrEgMDDQYXlgYCDS09ObfM+ECRNQWFiI3/3ud5AkCbW1tZg1a9YVT9OkpKRg6dKlzpRGREREnVSbX02zY8cOvPzyy3j77bdx8OBBfP7559iyZQtefPHFZt+zcOFClJaW2h/Z2dltXSYREREJ4lTLiJ+fH1QqFfLy8hyW5+XlISgoqMn3PP/885g0aRIeeeQRAED//v1hNBoxc+ZM/P3vf4dS2TgPabVaaLVaZ0ojIiKiTsqplhGNRoO4uDikpqbal1mtVqSmpiIhIaHJ91RWVjYKHCqVCgAgSZKz9RIREdENxqmWEQBITk7GlClTMHjwYAwdOhQrV66E0WjEtGnTAACTJ09GaGgoUlJSAACjR4/GihUrMHDgQMTHx+PUqVN4/vnnMXr0aHsoISIioq7L6TAybtw4FBQUYNGiRcjNzUVsbCy2bt1q79SalZXl0BLy3HPPQaFQ4LnnnsOFCxfg7++P0aNH46WXXmq9b0FERESdlkLqBOdKysrK4OnpidLSUhgMBtHlEBERUQu09PjNe9MQERGRUAwjREREJBTDCBEREQnFMEJERERCMYwQERGRUAwjREREJBTDCBEREQnFMEJERERCMYwQERGRUAwjREREJBTDCBEREQnFMEJERERCMYwQERGRUAwjREREJBTDCBEREQnFMEJERERCMYwQERGRUAwjREREJBTDCBEREQnFMEJERERCMYwQERGRUAwjREREJBTDCBEREQnFMEJERERCMYwQERGRUAwjREREJBTDCBEREQnFMEJERERCMYwQERGRUAwjREREJBTDCBEREQnFMEJERERCMYwQERGRUAwjREREJBTDCBEREQnFMEJERERCMYwQERGRUAwjREREJBTDCBEREQnFMEJERERCMYwQERGRUAwjREREJBTDCBEREQnFMEJERERCMYwQERGRUAwjREREJBTDCBEREQnFMEJERERCMYwQERGRUAwjREREJBTDCBEREQnFMEJERERCMYwQERGRUAwjREREJBTDCBEREQnFMEJERERCMYwQERGRUAwjREREJBTDCBEREQnFMEJERERCMYwQERGRUC6iCyAiImoTlhqg7CJQkQ9Asi1U2CaKy+bR9OtaA2AIBdS6Ni+3K2MYISKizkeSAGMhUJoNlF0ASs87PsouAOW5qA8h18k9EPAMA7zCbNPujvM6g3O1m8qAyiKgstg2Laqfr6kEFCpAqQSULrbnqvplDvMNpkoVoFACUMhTRd20btlly9HgdYUC6H4L4OrdOvvLSdcURlavXo3ly5cjNzcXMTExeOuttzB06NBm1y8pKcHf//53fP755yguLkaPHj2wcuVK3HvvvddcOBERdVCSBFSXNggH2fUBwVgAQGE7eLpcdiC1Las7qNY9r1uv6hJQVveZFwCL6eq1qDSAe5D8OXXBRJIaPLcX3eA127wkydusrQIq8uTHhf1Nb0fnCXh2rw8nHkFyqGgYMhqGDmvttezZtjX9WyBsiJBNOx1GNm7ciOTkZKxZswbx8fFYuXIlkpKSkJGRgYCAgEbrm81m3HXXXQgICMBnn32G0NBQnDt3Dl5eXq1RPxERXc5qlQ921hp5aqltel6y2P4qrjv4Kxv8pVy3TNVgWYNHZWHj1oiGD3N5O3xRha3FohvgGSqHAEOo47zeT/5e10qS5PBQkiWHqpLsBtMseVpdIoev6qNA3tGWf7baDdD7Anof29T20OgBq0X+97FaHJ9Llvp/X8nSeD1IgGSV65asTczDcb7hOlr3a99P10khSZJTbVjx8fEYMmQIVq1aBQCwWq0ICwvD3LlzsWDBgkbrr1mzBsuXL0d6ejrUavU1FVlWVgZPT0+UlpbCYHCiKYyIqLOz1ALlOXLfhzJbi0CZ7VF6QX6tptIxcNQddETT+9rCQZgtIHSTwwPQ4GBaazugWuuf21+zOq6nNdg+yxY4PEIAF43Y7wgApvIGIcUWWsrzAK1Hg5BxWeDQ+wBqV9GVt7mWHr+dCiNmsxl6vR6fffYZxo4da18+ZcoUlJSUYPPmzY3ec++998LHxwd6vR6bN2+Gv78/JkyYgGeeeQYqlarJ7ZhMJphM9c1vZWVlCAsLYxghopazWhv/5Vh3gLvackA+NaBysZ0qcAGU6vpTBip1/XJ7R8jLSBJgMdseNfK01lT/3L7cJD+vLqsPGGXn5fBRegGoyG2lcKGw1a2u/24KZf1fx1bbVKoLBpb6+aaotPWhoGHYqJs3hMp/4VOX1tIw4tRpmsLCQlgsFgQGBjosDwwMRHp6epPvOXPmDL777jtMnDgRX3/9NU6dOoXHHnsMNTU1WLx4cZPvSUlJwdKlS50pjYhuVFaL3AReWSyfv6+6BFQ1eN7c8urS9qnP3s/B9rDWyuHCWtN621CqAUMwYLCdfjCEOD7XuF8WkmzByWH+Ok9VNAwnkhVwcb2+zyRqoM2vprFarQgICMA///lPqFQqxMXF4cKFC1i+fHmzYWThwoVITk62z9e1jBDRDUiS5KseLp0FLmXapmeBYttzY37bbNehT0SDKxGUthbbhqcMLDXNtxBIFsBiaUFnSgXgopVbFFRquWOlSm1bpgE0braQYWttMITawkYo4BYg9sCvUMgtKURtxKlfl5+fH1QqFfLy8hyW5+XlISgoqMn3BAcHQ61WO5yS6dOnD3Jzc2E2m6HRND7fp9VqodVqnSmNiFqbpQbIPQJk7wWy9wDn98t9E3Re8pUDrl5Xfu5qm9d5AS46uVNjXdioCxqXMoFL5+SrFa5G4wHoveVLD129AVcfear3abzM1VvevkrdzGWQ13Bgr2sdqOsE6tAxtK5zqMXWQqFpHDaUTZ+WJiInw4hGo0FcXBxSU1PtfUasVitSU1MxZ86cJt8zfPhwrF+/HlarFUrb/wBOnDiB4ODgJoMIEQlSWVwfPLL3AhcONB0SKotaf9sKpdwa4B0BeIcDPrapd4TcWuDqLR/YRaprHWALAVGrc/q/quTkZEyZMgWDBw/G0KFDsXLlShiNRkybNg0AMHnyZISGhiIlJQUA8Oijj2LVqlV44oknMHfuXJw8eRIvv/wyHn/88db9JkTUclYrUHTSFjxs4aPwROP1dF5A2FAgLF6e6v3kyxirSmyXMl7teYncmgLI/Rq8IwCf8Pqg4W177tVdfNggImGcDiPjxo1DQUEBFi1ahNzcXMTGxmLr1q32Tq1ZWVn2FhAACAsLw7Zt2/Dkk09iwIABCA0NxRNPPIFnnnmm9b4FUVdRWQwUpAMFGfLlhA3HGnC4JLLB/OXLKovkgZuqLjX+fN9IOXh0j5envpHX31eh1iy3sGgNzV95QkRdmtPjjIjAcUaoy6m6BOSnAwXH66cFGfIIkK3FRQeExtlaPm4Bug0B3Hxb7/OJqMtrk0t7iagBSZJPbWT+IF/K6aKzPbRNPG9mWlslh4x8W9ioCx8Vuc1v17M74B8lD5ykdLns/hV1w2erLlvWYB21KxAyCAjq3zEGjCKiLo9hhMgZpnLgzE7g1LfAqVR5OOi2YugGBEQD/tFAQB/Avw/gf5M8qiMR0Q2EYYToSiQJyPsVOLVdDh9Zux1vcKXSAj0SADd/oLZaHmHzatOaKjjcSdQQKrd0+PexhY8+8rwzdwElIurEGEaILld1CTizo771ozzH8XWfnkDvu4DeiUD475wf8lqS5DE8aqvlS1oF3pyKiKgjYBihrsVqBcwV8ukWcwVgqgBMZfLz/HQ5gJzf5zjaposrEHGbHD56jwR8e11fDQqF3FeD/TWIiAAwjNCNwmqVT6Gkb5FvbW4qbxA4yuXQYbY9WsIvSg4fkYlA92GAWte29RMRdWEMI9S5FZ4CjmwAjmyUb93dUkoXeRAurUE+TaJxBzyCgF53AL1GAt492q5mIiJywDBCnU9lMfDr58DhDfIplTpaA9D3D/LVJxp3+aoTrUeD5+7y/U20HvKltRyAi4ioQ2AYoc6h1gyc/D/g8CfAiW31t2dXqOR+HDF/AqLulcfQICKiToVhhDouSZJv1nb4E+DYvx2HLw8aAMSMB/o/BLgHiKuRiIiuG8MIdSw1VUDhSeDkNvk0TNGp+tc8goH+f5RbQQJvFlcjERG1KoYRan+SJN9jpfCEHDwKT9Y/L82Gw4Bgaj3QZ7QcQCJul4c5JyJqhiRJqK6xotxUg4rqWlSYamGqtcJbr4G/hxYGnQsUHbC/WF3dVTUWSJIEN60LtC7KDllrW2AYobZTawKKz9iCxgn5ypfCE3Jrh6ms+ffpvICQgcCAh+UgwuHPidqdudaKClMtKqprHQ7sFaZalFfXorrGAlOtFeZaa4OpxT7f8HnDaY3VCo1KCY2Lsn562XNto9dU0LgoUWuRayq31VBRXWOvp662iupa1Fqbv/+r1kUJfw+t/HCXpwEeOvuyANvUz10LjUv9HaslSYKp1ooqswVGcy0qzRb5YZKfG821ttcsqDLX2qYWVNdYUFUjP6+qcZyvCx91r11OqQD0GhfoNSrbw/Zc6wI3jQquGhXc7K+7wFWjhMWKRvve/m9jscJUY7VPTRYrTDX1y9+bMhh9gsWM/MwwQq3LUgucTgUOfuTY0fRyCiXgHS7fot4vEvC7qX6q9+WVLtTpSJKE0qoa6NQq6NQdowXPapVQUlWDYqMJRRVmFBvNKDLK02KjGZcqzbawUWsPG0bbwd5caxVd/nVRKAB3jQvcdXILQ5HRjPJquZXk/KUqnL9UddXP8NaroXFRotJkQWWNBZYrhJy2YJVgD1ntodLcPttpCsMItY7iTODQ/wBpHzsOn641AL69HcOGX6Q8pLqLVly9RNfAapWQV16Ns4WVOFdkxNkieXrONjWa5b9u3TQq+Lhr4OOmha+bBj5uGvvUx00D38te02tU9uZ4SZJgtlhRXSP/1Sr/NW21/0Vd3WC+btmlSjlwFBvN9tBRFzau9/ip16jgrpUP6h62qZvGBa4aldyC4aKE1sXxub11wzate13rooRKqUCNRYLZctlf7xZro7/mL1+mcVHItWjVDvW4a13goZMfda/p1SoolY5/1FTXWFBQbkJ+uQkF5SYUVJhQUFYtTxsuLzeh1irhUmXTf0xpXJRwu6ylQq9WwU2rgqumvtVCr1HB1RZOXW3PXdUq6Bo8d22wjl5TH2Sraq7Q6mKyoNJc91p9S0y12QKVUuHw71D/b6Bs9G+kUSmhVddNVegdIO7WFAwjdO1qqoDj/wEOfgic/bF+uauP3Mdj4J+BgL5s5aA2Za614myREafyK3AyrwKnCipwMq8c5y9VQadWwqBTw8NVDYPOBQZXNQw6NQyuLrap43JP23KT7TPPFlUi67LQYWpBi4HRbIGxuArZxVf/6xuQTx24aV3sAaO1/wD30Lk0CEO2EOSugY9eIx/AGxzQ6w7m7rZTAS4q5dU30Eno1CqE+egR5nPl+0nVtSjll1ej1iJBr1HBTSsHML26ffaJHLq6ziG663xTaj05h4GD/wKOfgpUl9oWKuTRSwdOAqLvY6sHtbpKcy3OFBhxMr/cIXicK6pstvm8wgQUVphbtQ4XpQLdvF3Rw9cN4b56eeonT7t5u6K6xmprmWj61EiR7bXiCvm5yd7HonGdSoV8AK37y1mrVkLnIv81rbM916lV8NSrHVte3LT2Fhhvvcah7wNdnVKpsO9Lah8MI9QyVSXA0U3AoX/JYaSOZ5jcAhI7AfDqLqw8al91pxIqTfWd+Yymy6bmWvvr8hUC8l+cEuQLqqySHCCskgRJAiRIsErya4AEq1Vell9uwsm8Clwoab6VwV3rgt4B7ugd4I5I27SHrxtqrVaUVdWirKoGZdU1tmnD+Vp52vB5VQ1clEp099XXhw371A0hXror/mWsdVHB01WNCD+3Fu3HSrMFRRVmVNdabOFCCZ1GBZ2LCmqVostcTUFdG8MINc1qBUrOAfm/Ab9tlh+11fJrKo3c+jFwEtBzBC+3vcFUmmuRXVyF7OJKZF+qRHZxFbKKK3H+UiVKKmvs4aO9O/MBgK+bBr0aBA45fHgg0KBttYO2ZAtHl/c3aAsKhQJuWhe4daHmeKKm8L+Ars5qAS6dBQrSbY8MIP+4POZH7WV/iQb0BQZNBgaMA/Q+Qsql61djseJiidyfIftSJbKKK23BowrniytRZHTutEZdf4eGlx+6aW1TW+c+V7UKKqUCCgBQAEqF/Fzh8FwBhQJQQAGlQn5NoVDAS69GZIAHege4t0uzeV0dRNR+GEa6CkutLXQcbxA60uVxPyympt+j0spXv4QNAWL/DIQOYmfUdmSutaKgwoS8smrkl1Ujv1x+nlcm9/qvNltQa7XCYpVQa5UcpjWWy5Y3mDdbrLZTIc0z6Fzkjn7eeoT5uKK7jx7dfPTwc9NCr7WNbaBtv858RHRjYxi50ZWeB3a+ChzZWH+a5XIuOjl0+EcDAdHy1D9aHgeEp2DahNUqIbPIiJN5Fcgvr7YFDhPyyk324FHsZAuFM7QuSlvYcHUIHd285SsNPF3VbbZtIqLLMYzcqCoKgJ9WAPver2/5cHEF/G8C/PsA/lH14cOrB0NHG7tkNCPtfAnSskpwKLsEh7NLUFrVzIBwDahVCgR46BBg0CKwbmqQR4t017pApVTARamASqmAWqV0mHdR2uZV8jIXpRIqlQJaFyV83TTsGElEHQbDyI2m6hLw81vAL2uAGqO8rMdw4I6/A90TACWb1NuaudaK9NwypGWX4FBWCdKyS5BZaGy0ntZFieggDwR7utpDRoCHFgEGHQIN8hDV3no1QwMR3fAYRm4UpgpgzzvArrcAk23sj5BBwJ3PAb3uZF+PViZJEqpqLDCaLCirrsHxnDJ78Dh6obTJobQj/NwwMMwLsd29EBvmheggA8d/ICICw0jnV1MN7H8f+HEFUFkoLwvoK7eERN/HEHIVFquEzMIKHL1QiuziKvvYGPJNsOQhlitNtmndzbFMtai0jZvRHE9XNWLD5NAx0BY+vPQcQImIqCkMI52VpUYegGzncqD8orzMpycw4lmg3wPsA9KEWosVpwoqcOxCGY5dKMWxC6X4LacMlebGd8t0hl6jQi9/d3voiA3zQoSfG0+vEBG1EMNIZ2O1AEc/A3a8LF+qCwCGUOD2Z+RRUFW8CgKQx9I4kVeOXy+U4eiFUhy9UIrjOWVN3lfEVa3CzSEG9PJ3h4fO8RbddWNm1N38ymEMDa08SmZ7DI5FRHQjYxjpLCQJSP8P8N1/yeOEAICbP3Dr34C4aYBaJ7Y+gaprLDiRVy63eFyUWzzSc8phtjQOHu5aF/QNMaBfiCf6dzOgf6gnIvzcoWKgICIShmGkM8g9CnyzADj3kzyv8wSGPwEM/SugFXfLZxGMplocz7GdZrkoT0/lV6C2iaHJPXQu6BfiiX6hBvQL9UT/UE+E+7qxJYOIqINhGOnIjIVyS8jBDwHJKg9OljAbGPY44Ooluro2V1pZg18vluLYxVL8agseZwqNTXYc9dar0S/UE31D5NaO/qGeCPPWM3gQEXUCDCMdUa0Z2PcusOOV+st0b74fuOuFG+bOuDUWKwpsw5vnNxh1tG648zOFFcgubvourYEGLfqFeOLmUE/0CzHg5lBPhHjq2GGUiKiTYhjpaE5uB7YuBIpOyvNBA4BRrwA9homty0kF5SakZZcgt7Sq0T1V8suqW3wztjAfV9upFrnV4+YQAwI8um7/GCKiGxHDSEdRcALY9ixwars8r/cDRi4CBv65U1ymm1tajT2ZRfjlTDH2ZBbhTEHjEUcvVzfUub+HFoGXjUDazcsVN4d4wlPPq4OIiG50DCOiVZUAO18B9v4TsNYCSjVwyyzgtqfkjqodVHZxJfZkFmPPmSLsySxGVnGlw+sKBRAV6IEevvrLhjmXnwcadPByVbNPBxERMYwIY7XIHVO/+y+gskhedtMoIOklwLeX2NouI0kSzhVVYk9mEfacKcaezGJcKHHsz6FUADeHeCI+wgfxPX0xNNyHrRpERNQiDCMiZP4o9wvJOyrP+0UB97wM9E4UW1cDJZVm/HCyEDvS87HrdCHyykwOr6uUCvQP9UR8Tx/cEuGLuHBvGHQMH0RE5DyGkbZWVQLkHAZy0oCLafK0+Iz8ms5TvofM4L8IHzlVkiRk5JXju/R8fJ+ejwPnLqHh0B1qlQIx3bwQ39MH8RG+iOvhDTctfz5ERHT9eDRpTZXFjYNH3ZDtDSlUwOBp8n1k3Hzbt8YGqswW/Hy6EN+l52NHRkGjUy+RAe64MzoAt93kj0HdveGq6fgdaYmIqPNhGLlWVSXAxYP1oeNiGlByrul1vboDwbFASKxtOhDQ+7RToY6yiyvxfUY+vkvPx+7TRQ73atG4KDGsly/ujA7AHVEBCPPRC6mRiIi6FoYRZ0kScOh/gK+fAmqbGJTLO9wxeATHCAseAGCuteLAuUvYcUI+/XIir8Lh9RBPHe6IDsCd0QEY1suPrR9ERNTuGEacYTYCW+YDh9fL8149gNA4W/CIkR+u3kJLBOTWj50nCrDzRAF+PlUIo9lif02pAOJ6eNsDSFSgB0cuJSIioRhGWqrgBPDpZKDgOKBQAnc+Bwx/ElAqRVeG6hoL9mQWY2dGAXaeyMfpywYc83XT4Lab/DEiyh+33+QPL71GUKVERESNMYy0xJFNwP8+AdQYAfdA4MH3gYhbhZUjSRIyC43YkSG3fvxyxrHvh0qpwKDuXrj9Jn/cflMAbg4xcHAxIiLqsBhGrqSmGti2ENj/gTwfcZscRNwD2r0USZLww8lCbP8tFztPFDS6iVyQQWdv+RjW2w+erhzzg4iIOgeGkeYUZwKbpsiX6kIhD88+YkG73yem1mLFlqM5eGfHaaTnltuXa1RKDI3wkVs/ovwRGeDOvh9ERNQpMYw05fj/Al/OBkylgN4XeOCf7T46anWNBf8+eB7/2HnGft8Xd60Lxg4MwR1RAbilpy8HHSMiohsCj2YN1ZqBb5cAv6yW58PigYfWAp6h7VZChakW6/ecw3s/ZiK/XB6C3cdNg2nDwjE5IZz3eyEiohsOw0id0vPApmnA+b3yfMIcIHFJuw3Tfsloxtqfz+LDn8+itKoGABDsqcPM23pi3JAw6DX8pyIiohsTj3AAcHI78PlMoKpYvl/M2HeA6PvaZdM5pVV494dMfLI3C1U18nggPf3dMOv2XhgbGwqNi/hLh4mIiNpS1w4jllpgx8vAj6/L88GxwMMfyqOotrHMQiPW7DiNzw+dR41FviNdv1ADHhvRG0k3B0HFS3GJiKiL6LphxFwJrH8YOPujPD/kESDpZcBF26abPVtoxPL/y8A3R3Psd8WNj/DB7Dt649ZIP14RQ0REXU7XDSNqV8AjGNC4A6PfAPo/1Oab3HWqEI/+zwGUVdcCAEZGB+CxO3ohroe4e9cQERGJ1nXDiEIB/P6/gdufAfx6t/nm1u/JwqLNx1BrlTCwuxdSHuiP6CBDm2+XiIioo+u6YQQAtO6Atm2DiMUq4aUtx/HBrkwAwNjYECx7cAB0at4dl4iICOjqYaSNlVfX4IkNafguPR8A8Le7bsKcO3uzXwgREVEDDCNtJLu4Eo98uB8ZeeXQuiix4uFY3DcgWHRZREREHQ7DSBs4cK4Yf/3XARRWmBHgocW7kwcjJsxLdFlEREQdEsNIK/vy0AU8/dkRmC1W9A024P2pgxHs6Sq6LCIiog6LYaSVWK0SVn57Am9+dwoAcHffQKz8UyyHcSciIroKHilbQZXZgvmbDmPL0RwAwKMjeuGpu6Og5CiqREREV8Uwcp3yy6rxyEf7ceR8KdQqBV6+vz/+ODhMdFlERESdBsPIdTh2oRQzPtqPnNJqeOvV+MekwRgawdFUiYiInHFNt4RdvXo1wsPDodPpEB8fj71797bofRs2bIBCocDYsWOvZbMdyrZfc/HHNbuRU1qN3gHu+HL2cAYRIiKia+B0GNm4cSOSk5OxePFiHDx4EDExMUhKSkJ+fv4V33f27FnMnz8ft9566zUX21GcKajAYx8fRFWNBbdG+uHzx4ahh6+b6LKIiIg6JafDyIoVKzBjxgxMmzYNffv2xZo1a6DX6/HBBx80+x6LxYKJEydi6dKl6Nmz53UV3BF8n1EAi1XC4B7eWDt1CAw6teiSiIiIOi2nwojZbMaBAweQmJhY/wFKJRITE7F79+5m3/fCCy8gICAA06dPb9F2TCYTysrKHB4dyZ4zRQCAkX0C4aK6pjNdREREZOPUkbSwsBAWiwWBgYEOywMDA5Gbm9vke3766Se8//77ePfdd1u8nZSUFHh6etofYWEd5+oUq1XC3rPFAID4nuwjQkREdL3a9M/68vJyTJo0Ce+++y78/Pxa/L6FCxeitLTU/sjOzm7DKp1zIr8cJZU10GtU6B/qKbocIiKiTs+pS3v9/PygUqmQl5fnsDwvLw9BQUGN1j99+jTOnj2L0aNH25dZrVZ5wy4uyMjIQK9evRq9T6vVQqvVOlNau9lzRm4VievhDTVP0RAREV03p46mGo0GcXFxSE1NtS+zWq1ITU1FQkJCo/Wjo6Nx9OhRpKWl2R9/+MMfcMcddyAtLa1DnX5pqT2Zcn+ReF7GS0RE1CqcHvQsOTkZU6ZMweDBgzF06FCsXLkSRqMR06ZNAwBMnjwZoaGhSElJgU6nQ79+/Rze7+XlBQCNlncGkiRhb2ZdfxFfwdUQERHdGJwOI+PGjUNBQQEWLVqE3NxcxMbGYuvWrfZOrVlZWVAqb8zTF6cLKlBYYYbWRYkB3dhfhIiIqDUoJEmSRBdxNWVlZfD09ERpaSkMBoOwOv7nl3N47stjSOjpi09m3iKsDiIios6gpcfvG7MJo43syeQlvURERK2NYaSFJEmyD3YWH8H+IkRERK2FYaSFzhZVIr/cBI1KiYHdvUSXQ0REdMNgGGmhulaR2DAv6NQqwdUQERHdOBhGWuiXulM07C9CRETUqhhGWkCSpPrOq+wvQkRE1KoYRlogu7gKOaXVcFEqMKiHl+hyiIiIbigMIy3wi20I+AHdPKHXOD1OHBEREV0Bw0gL1N0cj0PAExERtT6GkRbgzfGIiIjaDsPIVVwoqcL5S1VQKRUYHM4wQkRE1NoYRq6ibnyRfqGecNeyvwgREVFrYxi5irr+IrfwFA0REVGbYBi5Cnt/EQ52RkRE1CYYRq4gr6waZ4sqoVSA/UWIiIjaCMPIFdQNAd83xACDTi24GiIiohsTw8gVcAh4IiKitscwcgV1V9JwfBEiIqK2wzDSjIJyE04XGKFQAEMZRoiIiNoMw0gz9tpO0UQFesBLrxFcDRER0Y2LYaQZdZf03sL70RAREbUphpFm2G+Ox1M0REREbYphpAnFRjMy8soBsL8IERFRW2MYaUJdf5HIAHf4umsFV0NERHRjYxhpAoeAJyIiaj8MI02o7y/CzqtERERtjWHkMqWVNTieWwaALSNERETtgWHkMvvOFkOSgJ5+bgjw0Ikuh4iI6IbHMHIZ9hchIiJqXwwjl+HN8YiIiNoXw0gD5dU1OHahFABbRoiIiNoLw0gD+89dglUCuvvoEezpKrocIiKiLoFhpAEOAU9ERNT+GEYa+OVMXedV9hchIiJqLwwjNkZTLY7W9RdhywgREVG7YRixOXDuEixWCaFergjz0Ysuh4iIqMtgGLGxjy/CVhEiIqJ2xTBiY++8ykt6iYiI2hXDCIAqswWHz5cA4GBnRERE7Y1hBMChrEuosUgINGjRw5f9RYiIiNoTwwiAX2xDwN/S0xcKhUJwNURERF0LwwiAPXXji/AUDRERUbvr8mGkusaCQ9klANh5lYiISIQuH0YOZ5fAXGuFn7sWPf3cRJdDRETU5XT5MLIns/6SXvYXISIian8MI7bBzm7hYGdERERCdOkwYq614sC5SwB4czwiIiJRunQYOXqhBNU1Vvi4aRAZ4C66HCIioi6pS4eRX2xDwA8NZ38RIiIiUbp0GGnYeZWIiIjE6NJhRKNSQOui5GBnREREArmILkCk96YMganWArWyS2cyIiIiobp0GAEArYtKdAlERERdGpsEiIiISCiGESIiIhKKYYSIiIiEYhghIiIioRhGiIiISCiGESIiIhKKYYSIiIiEYhghIiIioRhGiIiISCiGESIiIhLqmsLI6tWrER4eDp1Oh/j4eOzdu7fZdd99913ceuut8Pb2hre3NxITE6+4PhEREXUtToeRjRs3Ijk5GYsXL8bBgwcRExODpKQk5OfnN7n+jh07MH78eHz//ffYvXs3wsLCcPfdd+PChQvXXTwRERF1fgpJkiRn3hAfH48hQ4Zg1apVAACr1YqwsDDMnTsXCxYsuOr7LRYLvL29sWrVKkyePLlF2ywrK4OnpydKS0thMBicKZeIiIgEaenx26mWEbPZjAMHDiAxMbH+A5RKJCYmYvfu3S36jMrKStTU1MDHx6fZdUwmE8rKyhweREREdGNyKowUFhbCYrEgMDDQYXlgYCByc3Nb9BnPPPMMQkJCHALN5VJSUuDp6Wl/hIWFOVMmERERdSLtejXNsmXLsGHDBnzxxRfQ6XTNrrdw4UKUlpbaH9nZ2e1YJREREbUnF2dW9vPzg0qlQl5ensPyvLw8BAUFXfG9r732GpYtW4Zvv/0WAwYMuOK6Wq0WWq3WmdKIiIiok3KqZUSj0SAuLg6pqan2ZVarFampqUhISGj2fa+++ipefPFFbN26FYMHD772aomIiOiG41TLCAAkJydjypQpGDx4MIYOHYqVK1fCaDRi2rRpAIDJkycjNDQUKSkpAIBXXnkFixYtwvr16xEeHm7vW+Lu7g53d/dW/CpERETUGTkdRsaNG4eCggIsWrQIubm5iI2NxdatW+2dWrOysqBU1je4vPPOOzCbzXjooYccPmfx4sVYsmTJ9VVPREREnZ7T44yIwHFGiIiIOp82GWeEiIiIqLUxjBAREZFQDCNEREQkFMMIERERCcUwQkREREIxjBAREZFQDCNEREQkFMMIERERCcUwQkREREIxjBAREZFQDCNEREQkFMMIERERCcUwQkREREIxjBAREZFQDCNEREQkFMMIERERCcUwQkREREIxjBAREZFQDCNEREQkFMMIERERCcUwQkREREIxjBAREZFQDCNEREQkFMMIERERCcUwQkREREIxjBAREZFQDCNEREQkFMMIERERCcUwQkREREIxjBAREZFQDCNEREQkFMMIERERCcUwQkREREIxjBAREZFQDCNEREQkFMMIERERCcUwQkREREIxjBAREZFQDCNEREQkFMMIERERCcUwQkREREIxjBAREZFQDCNEREQkFMMIERERCcUwQkREREIxjBAREZFQDCNEREQkFMMIERERCcUwQkREREIxjBAREZFQDCNEREQkFMMIERERCcUwQkREREIxjBAREZFQDCNEREQkFMMIERERCcUwQkREREIxjBAREZFQDCNEREQkFMMIERERCcUwQkREREIxjBAREZFQDCNEREQk1DWFkdWrVyM8PBw6nQ7x8fHYu3fvFdfftGkToqOjodPp0L9/f3z99dfXVCwRERHdeJwOIxs3bkRycjIWL16MgwcPIiYmBklJScjPz29y/Z9//hnjx4/H9OnTcejQIYwdOxZjx47FsWPHrrt4IiIi6vwUkiRJzrwhPj4eQ4YMwapVqwAAVqsVYWFhmDt3LhYsWNBo/XHjxsFoNOI///mPfdktt9yC2NhYrFmzpkXbLCsrg6enJ0pLS2EwGJwpl4iIiARp6fHbxZkPNZvNOHDgABYuXGhfplQqkZiYiN27dzf5nt27dyM5OdlhWVJSEr788stmt2MymWAymezzpaWlAOQvRURERJ1D3XH7au0eToWRwsJCWCwWBAYGOiwPDAxEenp6k+/Jzc1tcv3c3Nxmt5OSkoKlS5c2Wh4WFuZMuURERNQBlJeXw9PTs9nXnQoj7WXhwoUOrSlWqxXFxcXw9fWFQqFotH5ZWRnCwsKQnZ3N0zhN4P65Ou6jq+M+ujruoyvj/rm6G20fSZKE8vJyhISEXHE9p8KIn58fVCoV8vLyHJbn5eUhKCioyfcEBQU5tT4AaLVaaLVah2VeXl5Xrc9gMNwQ/3hthfvn6riPro776Oq4j66M++fqbqR9dKUWkTpOXU2j0WgQFxeH1NRU+zKr1YrU1FQkJCQ0+Z6EhASH9QFg+/btza5PREREXYvTp2mSk5MxZcoUDB48GEOHDsXKlSthNBoxbdo0AMDkyZMRGhqKlJQUAMATTzyB22+/Ha+//jruu+8+bNiwAfv378c///nP1v0mRERE1Ck5HUbGjRuHgoICLFq0CLm5uYiNjcXWrVvtnVSzsrKgVNY3uAwbNgzr16/Hc889h2effRaRkZH48ssv0a9fv1b7ElqtFosXL250aodk3D9Xx310ddxHV8d9dGXcP1fXVfeR0+OMEBEREbUm3puGiIiIhGIYISIiIqEYRoiIiEgohhEiIiISqtOHkdWrVyM8PBw6nQ7x8fHYu3ev6JI6jCVLlkChUDg8oqOjRZcl1A8//IDRo0cjJCQECoWi0T2SJEnCokWLEBwcDFdXVyQmJuLkyZNiihXkavto6tSpjX5X99xzj5hiBUhJScGQIUPg4eGBgIAAjB07FhkZGQ7rVFdXY/bs2fD19YW7uzsefPDBRoM/3shaso9GjBjR6Hc0a9YsQRW3r3feeQcDBgywD2yWkJCAb775xv56V/z9dOowsnHjRiQnJ2Px4sU4ePAgYmJikJSUhPz8fNGldRg333wzcnJy7I+ffvpJdElCGY1GxMTEYPXq1U2+/uqrr+LNN9/EmjVrsGfPHri5uSEpKQnV1dXtXKk4V9tHAHDPPfc4/K4++eSTdqxQrJ07d2L27Nn45ZdfsH37dtTU1ODuu++G0Wi0r/Pkk0/if//3f7Fp0ybs3LkTFy9exAMPPCCw6vbVkn0EADNmzHD4Hb366quCKm5f3bp1w7Jly3DgwAHs378fd955J8aMGYNff/0VQBf9/Uid2NChQ6XZs2fb5y0WixQSEiKlpKQIrKrjWLx4sRQTEyO6jA4LgPTFF1/Y561WqxQUFCQtX77cvqykpETSarXSJ598IqBC8S7fR5IkSVOmTJHGjBkjpJ6OKD8/XwIg7dy5U5Ik+TejVqulTZs22dc5fvy4BEDavXu3qDKFunwfSZIk3X777dITTzwhrqgOxtvbW3rvvfe67O+n07aMmM1mHDhwAImJifZlSqUSiYmJ2L17t8DKOpaTJ08iJCQEPXv2xMSJE5GVlSW6pA4rMzMTubm5Dr8pT09PxMfH8zd1mR07diAgIABRUVF49NFHUVRUJLokYUpLSwEAPj4+AIADBw6gpqbG4XcUHR2N7t27d9nf0eX7qM7HH38MPz8/9OvXDwsXLkRlZaWI8oSyWCzYsGEDjEYjEhISuuzvp0PetbclCgsLYbFY7CO/1gkMDER6erqgqjqW+Ph4rFu3DlFRUcjJycHSpUtx66234tixY/Dw8BBdXoeTm5sLAE3+pupeI/kUzQMPPICIiAicPn0azz77LEaNGoXdu3dDpVKJLq9dWa1WzJs3D8OHD7ePKp2bmwuNRtPo5p5d9XfU1D4CgAkTJqBHjx4ICQnBkSNH8MwzzyAjIwOff/65wGrbz9GjR5GQkIDq6mq4u7vjiy++QN++fZGWltYlfz+dNozQ1Y0aNcr+fMCAAYiPj0ePHj3w6aefYvr06QIro87sT3/6k/15//79MWDAAPTq1Qs7duzAyJEjBVbW/mbPno1jx451+b5YV9LcPpo5c6b9ef/+/REcHIyRI0fi9OnT6NWrV3uX2e6ioqKQlpaG0tJSfPbZZ5gyZQp27twpuixhOu1pGj8/P6hUqkY9jPPy8hAUFCSoqo7Ny8sLN910E06dOiW6lA6p7nfD35RzevbsCT8/vy73u5ozZw7+85//4Pvvv0e3bt3sy4OCgmA2m1FSUuKwflf8HTW3j5oSHx8PAF3md6TRaNC7d2/ExcUhJSUFMTExeOONN7rs76fThhGNRoO4uDikpqbal1mtVqSmpiIhIUFgZR1XRUUFTp8+jeDgYNGldEgREREICgpy+E2VlZVhz549/E1dwfnz51FUVNRlfleSJGHOnDn44osv8N133yEiIsLh9bi4OKjVaoffUUZGBrKysrrM7+hq+6gpaWlpANBlfkeXs1qtMJlMXff3I7oH7fXYsGGDpNVqpXXr1km//fabNHPmTMnLy0vKzc0VXVqH8Le//U3asWOHlJmZKe3atUtKTEyU/Pz8pPz8fNGlCVNeXi4dOnRIOnTokARAWrFihXTo0CHp3LlzkiRJ0rJlyyQvLy9p8+bN0pEjR6QxY8ZIERERUlVVleDK28+V9lF5ebk0f/58affu3VJmZqb07bffSoMGDZIiIyOl6upq0aW3i0cffVTy9PSUduzYIeXk5NgflZWV9nVmzZolde/eXfruu++k/fv3SwkJCVJCQoLAqtvX1fbRqVOnpBdeeEHav3+/lJmZKW3evFnq2bOndNtttwmuvH0sWLBA2rlzp5SZmSkdOXJEWrBggaRQKKT/+7//kySpa/5+OnUYkSRJeuutt6Tu3btLGo1GGjp0qPTLL7+ILqnDGDdunBQcHCxpNBopNDRUGjdunHTq1CnRZQn1/fffSwAaPaZMmSJJknx57/PPPy8FBgZKWq1WGjlypJSRkSG26HZ2pX1UWVkp3X333ZK/v7+kVqulHj16SDNmzOhSfwA0tW8ASGvXrrWvU1VVJT322GOSt7e3pNfrpfvvv1/KyckRV3Q7u9o+ysrKkm677TbJx8dH0mq1Uu/evaWnnnpKKi0tFVt4O/nLX/4i9ejRQ9JoNJK/v780cuRIexCRpK75+1FIkiS1XzsMERERkaNO22eEiIiIbgwMI0RERCQUwwgREREJxTBCREREQjGMEBERkVAMI0RERCQUwwgREREJxTBCRJ2SQqHAl19+KboMImoFDCNE5LSpU6dCoVA0etxzzz2iSyOiTshFdAFE1Dndc889WLt2rcMyrVYrqBoi6szYMkJE10Sr1SIoKMjh4e3tDUA+hfLOO+9g1KhRcHV1Rc+ePfHZZ585vP/o0aO488474erqCl9fX8ycORMVFRUO63zwwQe4+eabodVqERwcjDlz5ji8XlhYiPvvvx96vR6RkZH46quv2vZLE1GbYBghojbx/PPP48EHH8Thw4cxceJE/OlPf8Lx48cBAEajEUlJSfD29sa+ffuwadMmfPvttw5h45133sHs2bMxc+ZMHD16FF999RV69+7tsI2lS5fi4YcfxpEjR3Dvvfdi4sSJKC4ubtfvSUStQPSd+oio85kyZYqkUqkkNzc3h8dLL70kSZJ819ZZs2Y5vCc+Pl569NFHJUmSpH/+85+St7e3VFFRYX99y5YtklKptN8BOCQkRPr73//ebA0ApOeee84+X1FRIQGQvvnmm1b7nkTUPthnhIiuyR133IF33nnHYZmPj4/9eUJCgsNrCQkJSEtLAwAcP34cMTExcHNzs78+fPhwWK1WZGRkQKFQ4OLFixg5cuQVaxgwYID9uZubGwwGA/Lz86/1KxGRIAwjRHRN3NzcGp02aS2urq4tWk+tVjvMKxQKWK3WtiiJiNoQ+4wQUZv45ZdfGs336dMHANCnTx8cPnwYRqPR/vquXbugVCoRFRUFDw8PhIeHIzU1tV1rJiIx2DJCRNfEZDIhNzfXYZmLiwv8/PwAAJs2bcLgwYPxu9/9Dh9//DH27t2L999/HwAwceJELF68GFOmTMGSJUtQUFCAuXPnYtKkSQgMDAQALFmyBLNmzUJAQABGjRqF8vJy7Nq1C3Pnzm3fL0pEbY5hhIiuydatWxEcHOywLCoqCunp6QDkK102bNiAxx57DMHBwfjkk0/Qt29fAIBer8e2bdvwxBNPYMiQIdDr9XjwwQexYsUK+2dNmTIF1dXV+O///m/Mnz8ffn5+eOihh9rvCxJRu1FIkiSJLoKIbiwKhQJffPEFxo4dK7oUIuoE2GeEiIiIhGIYISIiIqHYZ4SIWh3P/hKRM9gyQkREREIxjBAREZFQDCNEREQkFMMIERERCcUwQkREREIxjBAREZFQDCNEREQkFMMIERERCcUwQkREREL9P1RSXBPotid2AAAAAElFTkSuQmCC\n"},"metadata":{}}],"execution_count":12}]}